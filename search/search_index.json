{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"ARAI AI Agents","text":"<p>Welcome to the ARAI AI Agents documentation site! ARAI (Autonomous Responsive Intelligent Agent) is an advanced AI agent framework designed to create and manage autonomous AI agents. It provides a flexible and powerful system for building, deploying, and orchestrating AI agents for various applications.</p>"},{"location":"#documentation-structure","title":"Documentation Structure","text":"<p>We follow the Di\u00e1taxis documentation framework to organize content into four main sections:</p> <ol> <li>Tutorials \u2013 Step-by-step guides to get started with ARAI  </li> <li>How-To Guides \u2013 Practical guides for specific ARAI tasks  </li> <li>LLMS Explained \u2013 Conceptual guides about LLMs and how they work  </li> <li>ARAI AI Agents \u2013 Conceptual guides about ARAI\u2019s architecture and design</li> <li>Resources \u2013 Resources for learning how to program AI agents</li> <li>API Reference \u2013 Technical reference for ARAI\u2019s APIs and components  </li> <li>Prompt Reference \u2013 Technical reference for ARAI\u2019s prompts and templates</li> </ol> <p>For code examples, API references, and prompt references, refer to the dedicated pages:</p> <ul> <li>Code Examples</li> <li>API Reference</li> <li>Prompt Reference</li> </ul> <p>Quickly find what you\u2019re looking for by using the navigation links on top or in the sidebar.</p>"},{"location":"#project-overview","title":"Project Overview","text":"<p>ARAI simplifies creating and managing AI agents by providing:</p> <p>\ud83d\ude80 Multiple AI Models: Connect to OpenAI, Anthropic, or other LLMs. \u2699\ufe0f Modular Architecture: Plug and play new connectors (Discord, Twitter, Telegram, etc.) with minimal effort. \ud83d\udd17 Prompt Chaining: Build, modify, and chain prompts to accomplish complex tasks. \ud83e\udde0 Memory &amp; Templates: Store persistent context and quickly adapt to different use cases. [WIP] \ud83d\udcca CLI: Monitor and manage agent interactions in real-time.</p>"},{"location":"#acknowledgements","title":"Acknowledgements","text":"<p>This project builds upon various open-source technologies and research in the field of artificial intelligence and autonomous agents. Special thanks to all contributors and the broader AI research community for their ongoing work in advancing agent-based systems.</p> <p>Get Started by checking out the Tutorials or explore the How-To Guides for more specific tasks!</p>"},{"location":"about-llms/","title":"Explanation: Understanding LLMs, Prompt Engineering, and ARAI AI","text":"<p>ARAI AI Agents leverages advanced Large Language Models (LLMs) to produce non-repetitive, context-aware content through various strategies, including prompt chaining. This document provides a brief history of AI, explains how LLMs differ from other AI approaches, covers tokenization and prompts, and explores two popular prompt engineering techniques\u2014Prompt Chaining and Chain-of-Thought (CoT) Prompting. We\u2019ll also highlight ARAI\u2019s approach to orchestrating these methods.</p>"},{"location":"about-llms/#table-of-contents","title":"Table of Contents","text":"<ol> <li>A Brief History of AI</li> <li>How LLMs Differ from Other AI Approaches</li> <li>Tokens, Prompts &amp; Generation</li> <li>Two Paths to Take: Prompt Chaining vs. Chain-of-Thought Prompting</li> <li>Limitations of LLMs</li> <li>How ARAI AI Agents Leverage LLMs</li> <li>Conclusion</li> </ol>"},{"location":"about-llms/#1-a-brief-history-of-ai","title":"1. A Brief History of AI","text":"<ol> <li> <p>Symbolic AI (1950s\u20131980s)    Early AI largely relied on rule-based or \u201cexpert\u201d systems, manually encoding logic rather than learning from large datasets.</p> </li> <li> <p>Machine Learning (1980s\u20132000s)    Statistical algorithms (decision trees, SVMs, etc.) gained traction, but they often required extensive feature engineering and couldn\u2019t handle nuanced language tasks well.</p> </li> <li> <p>Deep Learning (2010s)    Neural networks scaled to many layers (\u201cdeep\u201d) ushered in a new era of success in fields like vision, speech, and basic NLP tasks, yet still had limitations with long-range text dependencies.</p> </li> <li> <p>Transformers &amp; LLMs (Late 2010s\u2013Present)    The Transformer architecture (\u201cAttention Is All You Need\u201d) revolutionized NLP. Large Language Models (LLMs) like GPT (OpenAI), BERT (Google), and others leverage massive datasets, enabling more context-aware and flexible text generation.</p> </li> </ol>"},{"location":"about-llms/#2-how-llms-differ-from-other-ai-approaches","title":"2. How LLMs Differ from Other AI Approaches","text":"<ol> <li> <p>Context Handling    LLMs excel at understanding and generating text with long-range context, unlike older models that quickly lost track of previous content.</p> </li> <li> <p>General-Purpose Functionality    Traditional AI is usually task-specific, whereas modern LLMs can adapt to various language-related tasks via well-structured prompts.</p> </li> <li> <p>Few-Shot &amp; Zero-Shot Learning    LLMs can tackle tasks with minimal examples, a huge leap from older machine learning methods requiring large labeled datasets for each new domain.</p> </li> <li> <p>Fluency &amp; Creativity    Transformer-based models produce coherent, contextually rich text that often feels natural and human-like.</p> </li> </ol>"},{"location":"about-llms/#3-tokens-prompts-generation","title":"3. Tokens, Prompts &amp; Generation","text":""},{"location":"about-llms/#31-tokenization","title":"3.1 Tokenization","text":"<ul> <li>What Is a Token?   A token can be a sub-word, punctuation, or symbol. LLMs process text sequentially in tokens.  </li> <li>Context Window   LLMs only have a finite context window (e.g., 2k\u201332k tokens). If your conversation exceeds that window, older context may be dropped or truncated.</li> </ul>"},{"location":"about-llms/#32-prompts","title":"3.2 Prompts","text":"<ul> <li>Prompt as an Instruction   A prompt is the text or instruction you give an LLM.  </li> <li>Prompt Engineering   The art of shaping prompts to achieve specific outputs is called prompt engineering. Effective prompts may include role instructions, examples, or constraints.</li> </ul>"},{"location":"about-llms/#4-two-paths-to-take-prompt-chaining-vs-chain-of-thought-prompting","title":"4. Two Paths to Take: Prompt Chaining vs. Chain-of-Thought Prompting","text":"<p>Prompt engineering is the process of writing prompts that guide artificial intelligence (AI) models (LLMs) to generate desired outputs. Two popular techniques often used to improve the quality and reliability of responses are Prompt Chaining and Chain-of-Thought (CoT) Prompting. Each technique offers unique advantages and suits different types of tasks.</p>"},{"location":"about-llms/#41-prompt-chaining","title":"4.1 Prompt Chaining","text":"<ul> <li> <p>Definition   In Prompt Chaining, you break down a complex task into a series of smaller prompts. Each prompt\u2019s output feeds into the next, ensuring the model carries forward relevant context and partial results.</p> </li> <li> <p>Use Case   Ideal for:</p> </li> <li>Sequential tasks (e.g., multi-episode stories, multi-step transformations).</li> <li>Maintaining context over multiple posts or interactions.</li> <li> <p>Structured workflows where each step refines or expands the content.</p> </li> <li> <p>Example </p> </li> <li>Prompt 1: \u201cGenerate a character backstory.\u201d  </li> <li>Prompt 2: \u201cUsing the backstory, outline a 5-episode arc.\u201d  </li> <li>Prompt 3: \u201cWrite the first episode\u2019s script referencing the outline.\u201d</li> </ul> <p>This approach is central to how ARAI orchestrates episodes and seasons while preventing repetitive content.</p>"},{"location":"about-llms/#42-chain-of-thought-cot-prompting","title":"4.2 Chain-of-Thought (CoT) Prompting","text":"<ul> <li> <p>Definition CoT Prompting encourages the model to write out its reasoning steps before giving a final answer. Instead of simply asking for a solution, you instruct the LLM to \u201cshow its work\u201d in a structured or step-by-step explanation.</p> </li> <li> <p>Use Case   Best for:</p> </li> <li>Complex problem-solving (math, logic puzzles, or multi-faceted questions).</li> <li>Diagnostic tasks (where seeing intermediate reasoning is valuable).</li> <li> <p>Ensuring correctness by revealing potential errors in the reasoning chain.</p> </li> <li> <p>Example </p> </li> <li>Prompt: \u201cExplain how to solve this math problem step by step, then provide the final answer.\u201d  </li> <li>The model outputs a reasoning chain (hidden or partially visible) and a final solution.</li> </ul> <p>In short, CoT is more about unveiling the reasoning process. Prompt Chaining is about splitting tasks into multiple sequential steps. ARAI primarily uses Prompt Chaining but can combine CoT for more in-depth reasoning within each step.</p>"},{"location":"about-llms/#5-limitations-of-llms","title":"5. Limitations of LLMs","text":"<ol> <li> <p>Hallucinations    LLMs may invent details when unsure, which can lead to plausible-sounding but incorrect answers.</p> </li> <li> <p>Context Window Constraints    They can only handle a limited token count at once. Exceeding that limit truncates older parts of the conversation.</p> </li> <li> <p>Lack of True Understanding    Despite generating sophisticated text, LLMs do not possess consciousness or genuine comprehension.</p> </li> <li> <p>Bias &amp; Ethical Concerns    LLMs can reflect biases present in their training data. Caution is advised, especially for public-facing content.</p> </li> <li> <p>Prompt Quality    Output is only as good as the prompt. Poorly structured requests yield suboptimal results.</p> </li> </ol>"},{"location":"about-llms/#6-how-arai-ai-agents-leverage-llms","title":"6. How ARAI AI Agents Leverage LLMs","text":"<p>ARAI AI Agents harness LLMs with a focus on Prompt Chaining for narrative-driven content. Some highlights:</p> <ol> <li>Story-First Content    ARAI uses a TV show or cinematic model\u2014seasons, episodes, scenes\u2014to maintain overarching context.  </li> <li>Chained Prompts </li> <li>Each step (episode, scene, or post) references the preceding step\u2019s output, ensuring continuity and preventing repetition.  </li> <li>Multi-Agent Collaboration </li> <li>ARAI can integrate multiple agent personalities, each guided by specialized prompts or constraints.  </li> <li>Chain-of-Thought (Optional) </li> <li>For specific logic or puzzle-based tasks, ARAI can enable CoT to capture the LLM\u2019s reasoning process more transparently.</li> </ol>"},{"location":"about-llms/#7-conclusion","title":"7. Conclusion","text":"<p>Modern Large Language Models give us unprecedented flexibility in text generation. By employing Prompt Chaining and, when necessary, Chain-of-Thought Prompting, we guide LLMs toward more coherent, context-rich outputs that serve both creative and analytical tasks.</p> <p>ARAI AI exemplifies these methods by: - Building multi-episode narratives that avoid redundancy. - Utilizing carefully structured prompts to maintain story context across seasons. - Embracing or bypassing CoT as required by the complexity of each scenario.</p> <p>Next Steps: - Check out our How-To Guides for environment setup and configuring your LLM keys. - Dive into tutorials for hands-on practice creating your first story-driven agent. - See the Reference docs for ARAI\u2019s APIs and modules.  </p> <p>Happy prompt engineering with ARAI AI!</p>"},{"location":"cli-guide/","title":"ARAI CLI Usage Guide","text":"<p>The ARAI AI Agents command-line interface (CLI) offers a straightforward way to manage your AI agents, create media content (seasons/episodes), and schedule or force posts. This guide will walk you through the main menu options and typical use cases.</p>"},{"location":"cli-guide/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Launching the CLI</li> <li>Main Menu Overview</li> <li>CLI Tips &amp; Tricks</li> <li>Common Workflows</li> <li>Exiting the CLI</li> <li>Troubleshooting</li> <li>Conclusion</li> </ol>"},{"location":"cli-guide/#1-launching-the-cli","title":"1. Launching the CLI","text":"<ol> <li>Activate your Conda environment (if using conda):    <pre><code>conda activate arai_ai_agents\n</code></pre></li> <li>Navigate to your <code>arai_ai_agents</code> folder:    <pre><code>cd arai_ai_agents\n</code></pre></li> <li>Run the CLI script:    <pre><code>python main.py\n</code></pre></li> <li>You should see a welcome prompt similar to:</li> </ol> <pre><code>=== Main Menu ===\nWelcome to ARAI Agents.\nPlease select an option:\n\nCurrent Agent: None\n\n= Agent Management =\n1. Select an existing Agent\n2. Create a new Agent\n\n= Media Management =\n3. Create a new Season\n4. Create Season posts\n\n= Scheduler Management =\n5. Start Scheduler\n6. Check posting status\n7. Force post now\n8. Pause/Resume posting\n\n= Miscellaneous =\n9. Exit\n\nEnter your choice (1-9):\n</code></pre>"},{"location":"cli-guide/#2-main-menu-overview","title":"2. Main Menu Overview","text":"<p>Below is a breakdown of each main menu option and what it does:</p>"},{"location":"cli-guide/#1-select-an-existing-agent","title":"1. Select an Existing Agent","text":"<p>If you have previously created AI agents, you can pick which one you want to work with. After you select an agent, the Current Agent will change from <code>None</code> to the chosen agent\u2019s name. - Why: This allows you to isolate tasks (creating posts, scheduling, etc.) for a specific agent.</p>"},{"location":"cli-guide/#2-create-a-new-agent","title":"2. Create a New Agent","text":"<p>This option launches a short wizard to build a new AI agent from scratch: - Steps:   1. Provide a name or handle for the agent.   2. Supply any extra info (e.g., personality traits, brand guidelines).   3. Confirm creation. - Once complete, the new agent appears in the list of available agents.</p>"},{"location":"cli-guide/#3-create-a-new-season","title":"3. Create a New Season","text":"<p>Seasons act like story arcs or thematic campaigns for your agent\u2019s content. Selecting this option: 1. Prompts you for a season name (e.g., \u201cSeason 1: The Great Launch\u201d). 2. Optionally requests a description or theme. 3. Creates a basic YAML or JSON entry to track this season\u2019s details.</p>"},{"location":"cli-guide/#4-create-season-posts","title":"4. Create Season Posts","text":"<p>After you have a season in place (or multiple), this option helps you generate posts (or \u201cepisodes\u201d) related to that season. You may be asked: 1. Which season you want to create posts for. 2. How many posts (scenes/episodes) you need. 3. Any special context or prompts. The CLI will then process prompts and batch-generate content (e.g., social media posts) for that season.</p>"},{"location":"cli-guide/#5-start-scheduler","title":"5. Start Scheduler","text":"<p>ARAI includes a scheduler that can automatically post content at set intervals: 1. When you start the scheduler, you\u2019ll see logs or debug info showing if posting is live or in \u201ctest mode.\u201d 2. The system will queue up any scheduled posts from the currently selected agent (or all agents, depending on your config).</p>"},{"location":"cli-guide/#6-check-posting-status","title":"6. Check Posting Status","text":"<p>Use this to see which posts are scheduled, how many have gone out, and whether any are pending or on hold. It helps confirm that your agent is indeed posting or waiting to post.</p>"},{"location":"cli-guide/#7-force-post-now","title":"7. Force Post Now","text":"<p>If you don\u2019t want to wait for the next scheduled slot, you can force the system to post immediately. This can be handy if: - You need to push out an urgent update. - You want to test how the post looks on your platform right away.</p>"},{"location":"cli-guide/#8-pauseresume-posting","title":"8. Pause/Resume Posting","text":"<p>Allows you to temporarily halt the scheduler without losing its state. When paused: - No new posts are sent to Twitter (or other connectors). - You can safely resume posting later when you\u2019re ready.</p>"},{"location":"cli-guide/#9-exit","title":"9. Exit","text":"<p>Closes the CLI. Any running background jobs or schedulers (if not paused or stopped) may continue operating until their next iteration, but the main interactive session ends.</p>"},{"location":"cli-guide/#3-cli-tips-tricks","title":"3. CLI Tips &amp; Tricks","text":"<ul> <li>Switching Agents: You can always go back to option <code>1</code> to select another agent at any time.</li> <li>Inspecting Logs: If something goes wrong (e.g., an error connecting to Twitter), check the CLI output or your log files in <code>configs/agent_folder/agentName_post_log.yaml</code> for debug information.</li> <li>Using the .env: Make sure your environment variables (Twitter keys, Gemini API key, etc.) are properly set in your <code>.env</code> file or system environment so the CLI can access them.</li> <li>In-Depth Configuration: If you prefer custom intervals, advanced scheduling, or specialized post flows, you can modify the relevant <code>.yaml</code> or <code>.json</code> config files in the <code>configs/</code> directory. The CLI reads from and writes to these files to maintain state.</li> </ul>"},{"location":"cli-guide/#4-common-workflows","title":"4. Common Workflows","text":"<ol> <li>Create &amp; Configure a New Agent </li> <li>Select option 2 at the main menu.  </li> <li>Provide agent details.  </li> <li> <p>Optionally, create a season (option 3) and season posts (option 4).  </p> </li> <li> <p>Schedule Posts for Automated Publishing </p> </li> <li>Start the scheduler with option 5.  </li> <li>Check the status with option 6.  </li> <li>If you want immediate posting, option 7 triggers it right away.  </li> <li> <p>Pause/Resume with option 8.</p> </li> <li> <p>Batch Generate Contextual Posts </p> </li> <li>Select or create an agent.  </li> <li>Use option 3 to define a thematic \u201cseason.\u201d  </li> <li>Then option 4 to generate multiple posts in one go.  </li> <li>Let the scheduler handle posting times, or force them out (option 7).</li> </ol>"},{"location":"cli-guide/#5-exiting-the-cli","title":"5. Exiting the CLI","text":"<p>When you\u2019re done: 1. Select 9 at the main menu or press <code>Ctrl + C</code> to exit the application. 2. Any unsaved changes in memory will typically be written out to config/log files when you exit.</p>"},{"location":"cli-guide/#6-troubleshooting","title":"6. Troubleshooting","text":"<ul> <li>CLI Not Launching: Double-check you\u2019ve activated your conda environment or installed the dependencies with <code>pip install -r requirements.txt</code>.</li> <li>Agent Not Appearing in Menu: The agent creation process may have failed or you forgot to confirm. Check your <code>/configs/agent_folder/</code> for a new <code>.yaml</code> or <code>.json</code>.</li> <li>No Posting: Confirm <code>twitter_live = True</code> in your main settings if you want real posts. Otherwise, they\u2019re logged to a local file.</li> <li>Crashes or Errors: Review the CLI output or logs for stack traces. If the system can\u2019t find your API keys, ensure your <code>.env</code> or environment variables are correctly set.</li> </ul>"},{"location":"cli-guide/#conclusion","title":"Conclusion","text":"<p>That\u2019s the basics of using the ARAI AI Agents CLI. From selecting or creating new agents, to generating multi-episode story arcs, to scheduling or forcing posts, the menu-based system keeps everything at your fingertips. If you have questions or encounter issues, check out the official GitHub repository or submit an Issue for assistance.</p> <p>Happy creating and scheduling with ARAI!</p>"},{"location":"code/","title":"Code Examples","text":""},{"location":"code/#code-annotation-examples","title":"Code annotation Examples","text":""},{"location":"code/#codeblocks","title":"Codeblocks","text":"<p>Some <code>code</code> goes here.</p>"},{"location":"code/#plain-codeblock","title":"Plain codeblock","text":"<p>A plain codeblock:</p> <pre><code>Some code here\ndef my_function():\n    print(\"Hello, world!\")\n</code></pre>"},{"location":"code/#code-for-a-specific-language","title":"Code for a specific language","text":"<p>Some more code with the <code>py</code> at the start:</p> <pre><code>def my_function():\n    print(\"Hello, world!\")\n</code></pre>"},{"location":"code/#with-title","title":"With title","text":"my_function.py<pre><code>def my_function():\n    print(\"Hello, world!\")\n</code></pre>"},{"location":"code/#with-title-and-line-numbers","title":"With title and line numbers","text":"my_function.py<pre><code>def my_function():\n    print(\"Hello, world!\")\n</code></pre>"},{"location":"code/#with-line-numbers-and-line-highlighting","title":"With line numbers and line highlighting","text":"my_function.py<pre><code>def my_function():\n    print(\"Hello, world!\")\n</code></pre>"},{"location":"code/#icons-and-emojis","title":"Icons and Emojis","text":""},{"location":"explanation/","title":"Explanation: ARAI\u2019s Prompt Chaining Approach","text":"<p>ARAI AI Agents employs a narrative-based prompt chaining methodology to create cohesive, non-repetitive, and engaging content\u2014ranging from tweets and social media posts to entire story arcs. This approach draws inspiration from Hollywood screenwriters, using seasons and episodes to structure continuous storylines and maintain context.</p>"},{"location":"explanation/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Core Concept</li> <li>Why \u201cPrompt Chaining\u201d?</li> <li>High-Level Workflow</li> <li>The Importance of a Large Context Window</li> <li>Example Flow</li> <li>Benefits of This Approach</li> <li>Conclusion</li> </ol>"},{"location":"explanation/#core-concept","title":"Core Concept","text":"<ol> <li>Non-Repetitive Storytelling </li> <li>Each generated post builds on previous context, avoiding repetitive language or ideas.  </li> <li> <p>The AI references a \u201cuniverse\u201d or \u201cworld Bible\u201d similar to what screenwriters use in film or television.</p> </li> <li> <p>TV Show &amp; Cinematic Structure </p> </li> <li>Stories are segmented into seasons and episodes.  </li> <li> <p>Each episode can be further divided into \u201cscenes,\u201d which are effectively individual posts or tweets.</p> </li> <li> <p>Prompt Chaining </p> </li> <li>Prompts are carefully crafted to pass relevant context (e.g., last episode events, the overall season arc).  </li> <li>Chained prompts ensure that each subsequent piece of content knows what has happened before and maintains consistency.</li> </ol>"},{"location":"explanation/#why-prompt-chaining","title":"Why \u201cPrompt Chaining\u201d?","text":"<p>Typical AI-generated content can become repetitive when prompts are not carefully managed. Prompt chaining solves this by:</p> <ul> <li> <p>Carrying Over Context   Each step (post, scene, or episode) includes details of what came before, preventing \u201cresetting\u201d or \u201cforgetting\u201d and ensuring a natural flow.</p> </li> <li> <p>Layering Background &amp; Universe Data   The AI \u201cknows\u201d the characters\u2019 personalities, the setting, and prior events. This leads to more believable and varied outputs.</p> </li> <li> <p>Batch &amp; Story-Based Generation   Instead of randomly generating single posts, ARAI processes entire sequences of posts together, referencing each other for narrative coherence.</p> </li> </ul>"},{"location":"explanation/#high-level-workflow","title":"High-Level Workflow","text":"<ol> <li>Character Background Sheets </li> <li> <p>The system prompts the AI to create detailed character profiles:  </p> <ul> <li>Universe &amp; Backstory (setting, tone, etc.)  </li> <li>Traits (personality, style, goals)  </li> <li>Emojis &amp; Hashtags they might use  </li> </ul> </li> <li> <p>Season &amp; Episode Creation </p> </li> <li>Season = a broad story arc (e.g., Season 1: \u201cThe Origin\u201d).  </li> <li>Episodes = subdivisions in each season (e.g., Episode 1: \u201cAwakening\u201d, Episode 2: \u201cNew Allies\u201d).  </li> <li> <p>This structure emulates a TV series format, providing an expansive canvas for narrative progression.</p> </li> <li> <p>Scene-to-Post Conversion </p> </li> <li>Each episode is broken down into \u201cscenes,\u201d which map directly to individual social media posts.  </li> <li> <p>Context injection includes:  </p> <ul> <li>Last episode\u2019s key events  </li> <li>Character developments  </li> <li>The overall \u201cseason\u201d summary  </li> </ul> </li> <li> <p>Batch Generation </p> </li> <li>ARAI then prompts the AI to generate multiple posts at once (or in succession) so they share context and maintain narrative continuity.  </li> <li> <p>This ensures each post references the correct timeline and plot details.</p> </li> <li> <p>Season Rollovers </p> </li> <li>After a season ends, context from that entire season is folded into the AI\u2019s prompt for the next season.  </li> <li>This preserves continuity across seasons, allowing characters to evolve over time.</li> </ol>"},{"location":"explanation/#the-importance-of-a-large-context-window","title":"The Importance of a Large Context Window","text":"<p>To make prompt chaining truly effective, the underlying AI model needs a large context window. The context window refers to the amount of text (measured in tokens) that the model can \u201cremember\u201d and consider when generating a response.</p> <ul> <li> <p>Why We Use <code>2.0 Experimental Advanced model in Gemini Advanced.</code>   By default, ARAI uses the <code>Gemini-Exp-1206</code> model because it offers a large context window. This is ideal for our narrative-driven approach because:</p> </li> <li> <p>Long-Term Memory:     The model can retain information from earlier parts of the conversation (e.g., details from previous episodes or seasons), which is crucial for maintaining consistency in long-running storylines.</p> </li> <li> <p>Complex Narrative Structures:     A larger context window allows the model to handle intricate plots, multiple characters, and evolving relationships within the narrative.</p> </li> <li> <p>Reduced Repetition:     With more context available, the model is less likely to fall back on repetitive phrases or generic responses.</p> </li> <li> <p>Model Selection and Context Window:   When choosing a model for ARAI, the size of the context window is a primary consideration. While a larger context window generally improves performance on complex tasks, it can also increase computational cost and latency. The <code>gemini-pro</code> model provides a good balance between context size and efficiency for our use case.</p> </li> </ul>"},{"location":"explanation/#example-flow","title":"Example Flow","text":"<ol> <li>Initialize a \u201cConcept of AI Agent\u201d <pre><code>Prompt: \n\"You are a comedic AI sidekick from a futuristic Mars colony. Generate a backstory \ndetailing your origin, personality traits, and comedic style. Also include an \nemoji palette and hashtags you frequently use.\"\n</code></pre></li> <li>Generate a Character Sheet </li> <li>AI responds with backstory, style notes, emojis, hashtags (<code>#MarsLife</code>, <code>#CosmicComedy</code>, etc.).</li> <li>Create a Season Plan </li> <li>Season 1: \u201cLaunch Day\u201d with 5 episodes.</li> <li>Episode 1 </li> <li>Provide the AI with scene outlines or beats to cover in episode 1.  </li> <li>AI generates Scenes 1, 2, 3, each as separate social media posts but referencing each other\u2019s details.</li> <li>Proceed to Episode 2 </li> <li>Summarize Episode 1 outcomes: \u201cIn Episode 1, your comedic AI discovered a stowaway on the Mars rocket\u2026\u201d  </li> <li>The AI crafts next scenes with knowledge of Episode 1\u2019s revelations.</li> <li>Season Finale </li> <li>Summaries of all episodes in Season 1 inform the Season 2 kickoff prompt.</li> </ol>"},{"location":"explanation/#benefits-of-this-approach","title":"Benefits of This Approach","text":"<ul> <li>Narrative Consistency: By continuously chaining prompts, the AI won\u2019t \u201cforget\u201d critical details and the story remains coherent.  </li> <li>Creative Expansion: You can easily scale from short comedic sketches to grand multi-season arcs.  </li> <li>Flexibility: Adapt the same system for comics, film scripts, or novel chapters, just by tweaking prompts.  </li> <li>Engagement: Social media followers can follow an unfolding storyline rather than seeing disjointed or repetitive posts.</li> </ul>"},{"location":"explanation/#conclusion","title":"Conclusion","text":"<p>ARAI AI Agents leverages a story-first, chain-of-thought approach to generating content. By structuring the process akin to Hollywood screenwriting and dividing it into seasons, episodes, and scenes, ARAI creates vibrant, interconnected narratives. Each step references previous context, preventing repetitive output and fostering deeper engagement for readers (or social media audiences).</p> <p>If you\u2019d like to learn more about setting up your environment or configuring connectors:</p> <ul> <li>Check our How-To Guides for environment &amp; API key setup  </li> <li>Look at the Tutorials for step-by-step instructions on building your first season-based storyline  </li> </ul> <p>Happy storytelling with ARAI!</p>"},{"location":"how-to-guides/","title":"How-To Guide: Setting up ARAI AI, Google Gemini, and Twitter","text":"<p>This guide walks you through the essential steps for installing ARAI AI Agents in a conda environment, configuring the Google Gemini API key, and authenticating a Twitter account for agent interactions. We also detail how environment variables are managed via a <code>.env</code> file using <code>python-dotenv</code>.</p>"},{"location":"how-to-guides/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Set Up a Conda Environment </li> <li>Install ARAI AI Agents </li> <li>Obtain a Google Gemini API Key </li> <li>Get a Twitter API Key </li> <li>Authenticate Your Twitter Account </li> <li>Debug Mode </li> <li>Processing Speed &amp; Wait Times </li> <li>Next Steps </li> <li>Troubleshooting</li> </ol>"},{"location":"how-to-guides/#1-set-up-a-conda-environment","title":"1. Set Up a Conda Environment","text":"<p>Conda environments keep dependencies isolated and help avoid conflicts with other projects on your system.</p> <ol> <li>Install or Update Conda </li> <li>Download and install Miniconda or Anaconda if you haven\u2019t already.</li> <li> <p>Ensure your conda is up to date:      <pre><code>conda update conda\n</code></pre></p> </li> <li> <p>Create a New Environment </p> </li> <li>In your terminal or Anaconda Prompt, run:      <pre><code>conda create --name arai_ai_agents python=3.11\n</code></pre></li> <li>Activate your new environment:      <pre><code>conda activate arai_ai_agents\n</code></pre></li> </ol> <p>You now have an isolated environment ready for ARAI AI Agents.</p>"},{"location":"how-to-guides/#2-install-arai-ai-agents","title":"2. Install ARAI AI Agents","text":"<ol> <li>Clone the Repository <pre><code>git clone https://github.com/arai-ai/arai_ai_agents.git\n</code></pre></li> <li>Navigate to the Project Directory <pre><code>cd arai_ai_agents\n</code></pre></li> <li>Install Dependencies    Within your <code>arai_ai_agents</code> folder, install the required packages:    <pre><code>pip install -r requirements.txt\n</code></pre></li> <li>Run the Main Script <pre><code>python main.py\n</code></pre>    This starts the primary ARAI AI Agents application, which includes agent configurations, connectors, and any prompt-chaining logic.</li> </ol>"},{"location":"how-to-guides/#3-obtain-a-google-gemini-api-key","title":"3. Obtain a Google Gemini API Key","text":"<p>(Note: The steps below are illustrative. Refer to Google\u2019s official documentation for the most up-to-date instructions.)</p> <ol> <li>Sign Up or Log In </li> <li> <p>Go to Google Cloud Console.</p> </li> <li> <p>Create a New Project (if needed)  </p> </li> <li> <p>Click Select a project \u2192 New Project. Give it a name, then create it.</p> </li> <li> <p>Enable the Gemini API </p> </li> <li>Navigate to APIs &amp; Services \u2192 Library.</li> <li> <p>Search for Gemini API (or equivalent) and enable it.</p> </li> <li> <p>Create Credentials </p> </li> <li>Under APIs &amp; Services \u2192 Credentials, click + CREATE CREDENTIALS \u2192 API key.</li> <li> <p>Copy the API key (e.g., <code>AIzaSyD...</code>).</p> </li> <li> <p>Set the Environment Variable </p> </li> <li>Either place it in your system environment:      <pre><code>export GEMINI_API_KEY=\"AIzaSyD...\"\n</code></pre></li> <li>Or place the key in your <code>.env</code> file at the root of the project:      <pre><code>GOOGLE_GEMINI_API_KEY=AIzaSyD...\n</code></pre></li> </ol> <p>(ARAI uses <code>python-dotenv</code> to automatically load variables from <code>.env</code> if configured.)</p>"},{"location":"how-to-guides/#4-get-a-twitter-api-key","title":"4. Get a Twitter API Key","text":"<p>To allow ARAI AI Agents to interact with Twitter, you need developer credentials.</p> <ol> <li>Apply for a Twitter Developer Account </li> <li> <p>Go to the Twitter Developer Portal and apply for a Developer account.</p> </li> <li> <p>Create a Project &amp; App </p> </li> <li> <p>Once approved, create a Project, then create an App within that project.</p> </li> <li> <p>Generate Keys &amp; Tokens </p> </li> <li>Under Keys and tokens, generate an API Key, API Secret Key, and Bearer Token (or OAuth 2.0 Client if required).</li> <li>Copy these values somewhere secure.</li> </ol> <p>Example <code>.env</code> variables:</p> <pre><code>TWITTER_API_KEY=xxxxxxxxxxxxxxxxxxx\nTWITTER_API_SECRET=yyyyyyyyyyyyyyyyyyyyyyyyy\nTWITTER_BEARER_TOKEN=AAAAAAAAAAAAAAAAAAAAAAA\n</code></pre>"},{"location":"how-to-guides/#5-authenticate-your-twitter-account","title":"5. Authenticate Your Twitter Account","text":"<p>ARAI AI Agents includes modules or scripts (e.g., <code>twitter_app_auth.py</code>) to handle Twitter OAuth or token-based authentication. Below is a typical approach:</p> <ol> <li> <p>Load Environment Variables    Make sure you have <code>python-dotenv</code> installed and your <code>.env</code> file in the project root. For example:    <pre><code>from dotenv import load_dotenv\nload_dotenv()  # This will load your .env variables\n</code></pre></p> </li> <li> <p>Locate the Auth File </p> </li> <li> <p>In <code>arai_ai_agents/auth/twitter_app_auth.py</code> (or a similar file):      <pre><code>import os\nfrom dotenv import load_dotenv\n\nload_dotenv()  # ensures environment variables are loaded\n\nTWITTER_API_KEY = os.getenv(\"TWITTER_API_KEY\")\nTWITTER_API_SECRET = os.getenv(\"TWITTER_API_SECRET\")\nTWITTER_BEARER_TOKEN = os.getenv(\"TWITTER_BEARER_TOKEN\")\n\n# Additional OAuth logic, if needed\n</code></pre></p> </li> <li> <p>Set Your Credentials </p> </li> <li> <p>Store your credentials in a <code>.env</code> file or environment variables. The code snippet above automatically pulls from your environment.</p> </li> <li> <p>Run the App </p> </li> <li> <p>When you run <code>python main.py</code>, the application will attempt to initialize the <code>TwitterConnector</code> (if configured), using your environment variables.</p> </li> <li> <p>Test Connectivity </p> </li> <li>Use the connector or a test script to verify you can post or retrieve tweets.  </li> <li> <p>Example:      <pre><code>from arai_ai_agents.connectors.twitter_connector import TwitterConnector\nimport os\n\nconnector = TwitterConnector(\n    api_key=os.getenv(\"TWITTER_API_KEY\"),\n    api_secret=os.getenv(\"TWITTER_API_SECRET\"),\n    bearer_token=os.getenv(\"TWITTER_BEARER_TOKEN\")\n)\nconnector.test_connection()\n</code></pre></p> </li> <li> <p>Twitter Access Tokens </p> </li> <li>To post on behalf of a user, you will need access tokens for the specific Twitter account.</li> <li>The <code>twitter_app_auth.py</code> file may contain a function to fetch or handle these tokens.</li> <li> <p>Save your access tokens in the <code>.env</code> file:      <pre><code>TWITTER_ACCESS_TOKEN=XXXXXXXX\nTWITTER_ACCESS_TOKEN_SECRET=YYYYYYYY\n</code></pre></p> </li> <li> <p>Enable Twitter Live Mode </p> </li> <li>Set <code>twitter_live = True</code> in the <code>main.py</code> file so you can post to Twitter live:      <pre><code>twitter_live = True\n</code></pre></li> <li>Once successfully authenticated, your ARAI agents can interact with Twitter\u2014posting tweets, reading mentions, or replying to DMs, depending on your configuration.</li> </ol>"},{"location":"how-to-guides/#6-debug-mode","title":"6. Debug Mode","text":"<p>By default, posting to Twitter is disabled so you can see generated tweets in log files without actually publishing. This is by design so that you can test AI output before using real APIs:</p> <ul> <li>Log-Only Mode   The system logs tweets to a <code>yaml</code> file, typically located at <code>configs/agent_folder/agentName_post_log.yaml</code>.  </li> <li>Trial Run   Check this file to verify the AI is generating appropriate, non-repetitive content.  </li> <li>Switch to Live   Once you\u2019re satisfied, enable <code>twitter_live = True</code> to start posting live.</li> </ul>"},{"location":"how-to-guides/#7-processing-speed-wait-times","title":"7. Processing Speed &amp; Wait Times","text":"<p>Depending on your prompt complexity, ARAI AI may take a few minutes to generate a response\u2014especially if the agent is creating long-form or multiple pieces of content. The system is waiting for the LLM (e.g., Google Gemini) to process and return a detailed answer.</p> <ul> <li>CLI Feedback   Keep an eye on your command-line interface (CLI). You\u2019ll see logging messages that indicate whether the agent is still running or if an error occurs.  </li> <li>Batch Generation   If you\u2019re generating content in batches (e.g., multiple tweets or entire episodes), expect longer wait times as the AI compiles all required context.  </li> <li>Crash Handling   If the CLI exits unexpectedly or logs an error, it\u2019s likely that the model or process has crashed. You can check the logs for details and rerun once you\u2019ve addressed the issue.</li> </ul> <p>Reminder: The more context (characters, episodes, story arcs) we include, the more time the model may need to process your request.</p>"},{"location":"how-to-guides/#8-next-steps","title":"8. Next Steps","text":"<ul> <li>Using ARAI with Other Models: Check out the API Reference Documentation for integrating additional LLMs like OpenAI or Anthropic.</li> <li>Setting Up Additional Connectors: See our How-To Guides for adding Discord, Slack, or Telegram connectors.</li> <li>Managing Prompts &amp; Templates: Explore the Prompt Reference for advanced usage of prompt chaining and template customization.</li> </ul>"},{"location":"how-to-guides/#9-troubleshooting","title":"9. Troubleshooting","text":"<ul> <li> <p>Conda Environment Not Found   Ensure you spelled the environment name correctly or re-run <code>conda activate arai_ai_agents</code>.</p> </li> <li> <p>Credential Errors   Double-check environment variables are set in your <code>.env</code> or system variables. Make sure you restart the shell if you updated <code>.env</code>.</p> </li> <li> <p>Authentication Failures   Validate your Google or Twitter keys/tokens in their respective developer dashboards.</p> </li> </ul> <p>If you run into any issues, feel free to open an Issue.</p> <p>That\u2019s it! You\u2019ve now set up your environment, installed ARAI AI Agents, obtained the necessary API keys, and authenticated your Twitter account. Remember that complex or large-scale AI tasks may take a bit longer to generate responses, so don\u2019t worry if you see some delay\u2014it\u2019s just the model crafting detailed content.  </p> <p>Happy building with ARAI AI Agents!</p>"},{"location":"resources/","title":"AI Starter Resource Guide","text":"<p>Welcome to the AI Starter Resource Guide! This document provides a curated set of resources to help you begin (or continue) your journey into artificial intelligence (AI). It covers everything from Python programming and environment management to popular integrated development environments (IDEs) and AI-specific tools.</p>"},{"location":"resources/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Python Basics </li> <li>Conda for Environment Management </li> <li>Code Editors &amp; IDEs <ul> <li>Visual Studio Code </li> <li>Cursor AI </li> </ul> </li> <li>Foundational AI Resources </li> <li>Machine Learning &amp; Deep Learning Frameworks </li> <li>Online Courses &amp; Tutorials </li> <li>Communities &amp; Forums </li> <li>Additional References</li> </ol>"},{"location":"resources/#1-python-basics","title":"1. Python Basics","text":"<p>Python is the de facto language for AI. Here are a few beginner-friendly resources:</p> <ul> <li> <p>Official Python Docs https://docs.python.org/3/   The official documentation for Python. Great place to reference built-in modules, syntax details, and best practices.</p> </li> <li> <p>Automate the Boring Stuff with Python https://automatetheboringstuff.com/   A free online book that introduces Python through practical tasks and examples.</p> </li> <li> <p>Python Crash Course by Eric Matthes Amazon Link (not free, but highly recommended).   Teaches programming concepts, projects, and fundamental Python skills.</p> </li> </ul>"},{"location":"resources/#2-conda-for-environment-management","title":"2. Conda for Environment Management","text":"<p>Conda is a widely-used tool for managing virtual environments, especially useful when juggling multiple data science projects or conflicting library versions.</p> <ul> <li> <p>Installing Miniconda or Anaconda https://docs.conda.io/en/latest/miniconda.html   Miniconda is a minimal environment, while Anaconda is a more extensive distribution including many data science packages.</p> </li> <li> <p>Conda Cheat Sheet https://docs.conda.io/projects/conda/en/latest/user-guide/cheatsheet/   Quick reference for common commands (create envs, install packages, etc.).</p> </li> <li> <p>Practical Tips </p> </li> <li><code>conda create --name myenv python=3.9</code> \u2013 Create a new environment.  </li> <li><code>conda activate myenv</code> \u2013 Activate the new environment.  </li> <li><code>conda install numpy</code> \u2013 Install a package into the active environment.</li> </ul>"},{"location":"resources/#3-code-editors-ides","title":"3. Code Editors &amp; IDEs","text":"<p>A comfortable coding environment makes learning AI more enjoyable and productive.</p>"},{"location":"resources/#visual-studio-code","title":"Visual Studio Code","text":"<ul> <li> <p>VS Code https://code.visualstudio.com/   A free, lightweight yet powerful editor with a robust extension ecosystem.</p> </li> <li> <p>Python Extension https://marketplace.visualstudio.com/items?itemName=ms-python.python   Adds support for Python syntax, IntelliSense, debugging, linting, and more.</p> </li> <li> <p>Remote Development https://code.visualstudio.com/docs/remote/remote-overview   Work in containers, WSL, or remote machines \u2014 useful for data-intensive AI projects.</p> </li> </ul>"},{"location":"resources/#cursor-ai","title":"Cursor AI","text":"<ul> <li> <p>Cursor AI https://www.cursor.so/   A specialized code editor powered by AI. Cursor AI can provide in-editor suggestions, code completions, and debugging help tailored for data science and machine learning code.</p> </li> <li> <p>Setup &amp; Documentation   The site offers guides on how to integrate AI-based coding assistance into your workflow.  </p> </li> </ul>"},{"location":"resources/#4-foundational-ai-resources","title":"4. Foundational AI Resources","text":"<ul> <li> <p>Andrew Ng\u2019s \u201cAI Transformation Playbook\u201d https://landing.ai/ai-transformation-playbook/   A high-level overview of how companies adopt AI, also helpful to understand AI project lifecycles.</p> </li> <li> <p>Stanford\u2019s CS229: Machine Learning https://cs229.stanford.edu/   Lecture materials, notes, and assignments from one of the most popular ML courses.</p> </li> </ul>"},{"location":"resources/#5-machine-learning-deep-learning-frameworks","title":"5. Machine Learning &amp; Deep Learning Frameworks","text":"<ul> <li> <p>TensorFlow https://www.tensorflow.org/   An end-to-end open-source platform for machine learning from Google. Popular for deep learning, also supports a wide range of machine learning tasks.</p> </li> <li> <p>PyTorch https://pytorch.org/   A popular framework by Meta (Facebook). Known for its dynamic computation graph and ease of experimentation. Favored by many researchers.</p> </li> <li> <p>scikit-learn https://scikit-learn.org/stable/   Perfect for traditional machine learning algorithms. Great documentation and easy to integrate into Python projects.</p> </li> <li> <p>Hugging Face https://huggingface.co/   A platform &amp; library for state-of-the-art NLP and other ML tasks. With Transformers, you can quickly experiment with large language models (LLMs).</p> </li> </ul>"},{"location":"resources/#6-online-courses-tutorials","title":"6. Online Courses &amp; Tutorials","text":"<ul> <li>Coursera </li> <li>\u201cMachine Learning\u201d by Andrew Ng (classic intro course).  </li> <li> <p>\u201cDeep Learning Specialization\u201d by Andrew Ng.</p> </li> <li> <p>fast.ai https://www.fast.ai/   Practical deep learning courses that don\u2019t require advanced math prerequisites.</p> </li> <li> <p>Kaggle https://www.kaggle.com/   Hosts ML competitions. Offers free data sets and interactive tutorials (Kaggle Learn). Great for hands-on practice and portfolio building.</p> </li> </ul>"},{"location":"resources/#7-communities-forums","title":"7. Communities &amp; Forums","text":"<ul> <li> <p>Reddit /r/MachineLearning https://www.reddit.com/r/MachineLearning/   News, papers, and discussions on ML.</p> </li> <li> <p>Stack Overflow https://stackoverflow.com/   Essential Q&amp;A site for programming issues.</p> </li> <li> <p>Hugging Face Forums https://discuss.huggingface.co/   Focused on Transformers, NLP, and specialized model usage.</p> </li> <li> <p>Discord Communities   Many open-source ML or AI project communities have active Discord servers. For instance, Hugging Face, PyTorch, etc.</p> </li> </ul>"},{"location":"resources/#8-additional-references","title":"8. Additional References","text":"<ul> <li> <p>Papers with Code https://paperswithcode.com/   Tracks the latest in AI research along with code implementations.</p> </li> <li> <p>Arxiv https://arxiv.org/   The go-to place for preprints on ML, NLP, CV (computer vision), and other AI research fields.</p> </li> <li> <p>YouTube Channels </p> </li> <li>3Blue1Brown: Explains math and ML concepts visually.  </li> <li>Two Minute Papers: Summaries of recent AI papers.  </li> </ul>"},{"location":"resources/#final-notes","title":"Final Notes","text":"<ul> <li>Practice: The best way to learn AI is by doing. Try small projects on Kaggle or your own dataset.  </li> <li>Stay Updated: AI research moves quickly. Follow conferences like NeurIPS, ICLR, ICML, and domain-specific communities.  </li> <li>Experiment: Tools like VS Code or Cursor AI can speed up your development and debugging. Combine them with Conda to keep your environment clean.  </li> </ul> <p>We wish you the best on your AI journey! Remember that the AI field is broad and constantly evolving\u2014there\u2019s always something new to learn or try. Happy coding!</p>"},{"location":"tutorials/","title":"Getting Started with the Gemini API using Python","text":"<p>This tutorial will guide you through the process of setting up and making your first request to the Gemini API using the Google's Generative AI Python library.</p>"},{"location":"tutorials/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Prerequisites</li> <li>Configure Your API Key</li> <li>Initialize the Model</li> <li>Make Your First Request</li> <li>Using the Chat Interface</li> <li>Exploring Further</li> <li>Troubleshooting</li> <li>Conclusion</li> </ol>"},{"location":"tutorials/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, make sure you have the following:</p> <ul> <li> <p>Python 3.11+ installed   You can check your Python version by running:   <pre><code>python --version\n</code></pre>   or:   <pre><code>python3 --version\n</code></pre></p> </li> <li> <p>A Google Cloud Project   If you don't have one, create a new project in the Google Cloud Console.</p> </li> <li> <p>A Gemini API Key </p> </li> <li>Go to Google AI Studio.</li> <li>Click on \"Get API Key\".</li> <li>Select your Google Cloud Project where you want to enable the API.</li> <li> <p>Click on \"Create API Key\". Copy this key; you'll need it later.</p> </li> <li> <p>The <code>google-generativeai</code> Python library   You can install it using pip:   <pre><code>pip install google-generativeai\n</code></pre></p> </li> </ul>"},{"location":"tutorials/#configure-your-api-key","title":"Configure Your API Key","text":"<p>The <code>google-generativeai</code> library needs your API key to authenticate your requests to the Gemini API. There are a couple of ways to provide it:</p>"},{"location":"tutorials/#method-1-environment-variable-recommended","title":"Method 1: Environment Variable (Recommended)","text":"<ol> <li> <p>Set an environment variable named <code>GOOGLE_API_KEY</code> with your API key as the value.</p> </li> <li> <p>Linux/macOS: <pre><code>export GOOGLE_API_KEY=\"YOUR_API_KEY\"\n</code></pre></p> </li> <li> <p>Windows: <pre><code>setx GOOGLE_API_KEY \"YOUR_API_KEY\"\n</code></pre> (You might need to restart your console or IDE for it to take effect.)</p> </li> <li> <p>Then in your Python code, configure like this:    <pre><code>import google.generativeai as genai\nimport os\n\ngenai.configure(api_key=os.environ.get(\"GOOGLE_API_KEY\"))\n</code></pre></p> </li> </ol>"},{"location":"tutorials/#method-2-directly-in-your-code-less-secure","title":"Method 2: Directly in Your Code (Less Secure)","text":"<p>You can set your API key directly in your Python code. However, this is less secure, especially if you are sharing or versioning your code.</p> <pre><code>import google.generativeai as genai\n\ngenai.configure(api_key=\"YOUR_API_KEY\")  # Replace \"YOUR_API_KEY\" with your actual key\n</code></pre>"},{"location":"tutorials/#initialize-the-model","title":"Initialize the Model","text":"<p>Now, let's initialize the <code>GenerativeModel</code> with the <code>gemini-pro</code> model:</p> <pre><code>import google.generativeai as genai\n\n# ... API key configuration (see \"Configure Your API Key\") ...\n\nmodel = genai.GenerativeModel(\"gemini-pro\")\n</code></pre>"},{"location":"tutorials/#make-your-first-request","title":"Make Your First Request","text":"<p>Let's send a simple prompt to the Gemini API and get a response:</p> <pre><code>import google.generativeai as genai\n\n# ... API key configuration and model initialization (see \"Configure Your API Key\" &amp; \"Initialize the Model\") ...\n\nprompt = \"What is the capital of France?\"\n\nresponse = model.generate_content(prompt)\n\nprint(response.text)\n</code></pre> <p>Expected Output: <pre><code>The capital of France is Paris.\n</code></pre> (Or a similar, more elaborate response.)</p> <p>Explanation: - prompt: This variable holds the text prompt you are sending to the model. - model.generate_content(prompt): This calls the API, sending the prompt and receiving the generated content. - response.text: This accesses the text part of the response from the model.</p>"},{"location":"tutorials/#using-the-chat-interface","title":"Using the Chat Interface","text":"<p>The Gemini API also supports a chat interface where you can have back-and-forth conversations.</p> <pre><code>import google.generativeai as genai\n\n# ... API key configuration (see \"Configure Your API Key\") ...\n\nmodel = genai.GenerativeModel(\"gemini-pro\")\n\nmessages = []\n\nmessages.append({\n    \"role\": \"user\",\n    \"parts\": [\"What is the capital of France?\"]\n})\n\nresponse = model.generate_content(messages)\nprint(response.text)\n\n# Add model's response back into the conversation history\nmessages.append({\n    \"role\": \"model\",\n    \"parts\": [response.text]\n})\n\n# User asks another question\nmessages.append({\n    \"role\": \"user\",\n    \"parts\": [\"And what is its population?\"]\n})\n\nresponse = model.generate_content(messages)\nprint(response.text)\n</code></pre>"},{"location":"tutorials/#explanation","title":"Explanation","text":"<ul> <li>messages: This list stores the history of your conversation.</li> <li>Adding messages: Each item in <code>messages</code> is a dictionary with:</li> <li>role: Either <code>\"user\"</code> or <code>\"model\"</code>.</li> <li>parts: A list of strings (or other content parts) representing the message content.</li> <li>model.generate_content(messages): Takes the entire message history to provide context for the model.</li> <li>Appending the model's response: To maintain conversation flow, you append the AI's response to <code>messages</code> so it \u201cremembers\u201d earlier turns.</li> </ul>"},{"location":"tutorials/#exploring-further","title":"Exploring Further","text":"<ul> <li> <p>More Model Parameters   Check out the Google AI for Developers documentation to learn about additional parameters (e.g., <code>temperature</code>, <code>top_k</code>, <code>top_p</code>) for controlling the model's generation.</p> </li> <li> <p>Safety Settings   You can configure safety settings to control what type of content the model generates. See the Safety Settings documentation.</p> </li> <li> <p>Other Models   Explore other available models, such as <code>gemini-pro-vision</code> for multimodal input (text + images).</p> </li> </ul>"},{"location":"tutorials/#troubleshooting","title":"Troubleshooting","text":"<ul> <li> <p>API key not found error   Make sure your API key is correctly configured as an environment variable or in your code.</p> </li> <li> <p>PermissionDenied error   Verify that the API key is associated with a Google Cloud project that has the Gemini API enabled.</p> </li> <li> <p>Other errors   Refer to the Gemini API documentation for more detailed error messages and troubleshooting steps.</p> </li> </ul>"},{"location":"tutorials/#conclusion","title":"Conclusion","text":"<p>Congratulations! You've now successfully made your first request to the Gemini API and even tried out a simple conversation. This is just the beginning of what you can do with this powerful API. Explore the documentation and experiment with different prompts and model parameters to unlock the full potential of Google's Generative AI models. ```</p>"},{"location":"api/SUMMARY/","title":"SUMMARY","text":"<ul> <li>auth<ul> <li>twitter_app_auth</li> </ul> </li> <li>connectors<ul> <li>twitter_connector</li> </ul> </li> <li>main</li> <li>models<ul> <li>base_model</li> <li>gemini_model</li> </ul> </li> <li>prompt_chaining<ul> <li>step_1</li> <li>step_2</li> <li>step_3</li> </ul> </li> <li>utils<ul> <li>content_generator</li> <li>post_manager</li> <li>template_types</li> </ul> </li> </ul>"},{"location":"api/main/","title":"main","text":""},{"location":"api/main/#arai_ai_agents.main","title":"<code>arai_ai_agents.main</code>","text":""},{"location":"api/main/#arai_ai_agents.main.list_available_agents","title":"<code>list_available_agents()</code>","text":"<p>List all available agent configs in the configs folder</p> <p>Returns:</p> Name Type Description <code>list</code> <p>List of available agent names</p> Source code in <code>arai_ai_agents/main.py</code> <pre><code>def list_available_agents():\n    \"\"\"List all available agent configs in the configs folder\n\n    Returns:\n        list: List of available agent names\n    \"\"\"\n    agents = []\n    configs_dir = \"configs\"\n    if os.path.exists(configs_dir):\n        for item in os.listdir(configs_dir):\n            if os.path.isdir(os.path.join(configs_dir, item)):\n                agents.append(item)\n    return agents\n</code></pre>"},{"location":"api/main/#arai_ai_agents.main.list_available_seasons","title":"<code>list_available_seasons(agent_name)</code>","text":"<p>List all available seasons for an agent</p> <p>Parameters:</p> Name Type Description Default <code>agent_name</code> <code>str</code> <p>The name of the agent to list seasons for</p> required <p>Returns:</p> Name Type Description <code>list</code> <p>List of available season names</p> Source code in <code>arai_ai_agents/main.py</code> <pre><code>def list_available_seasons(agent_name):\n    \"\"\"List all available seasons for an agent\n\n    Args:\n        agent_name (str): The name of the agent to list seasons for\n\n    Returns:\n        list: List of available season names\n    \"\"\"\n    seasons = []\n    config_path = os.path.join(\"configs\", agent_name)\n    if os.path.exists(config_path):\n        for item in os.listdir(config_path):\n            if os.path.isdir(os.path.join(config_path, item)):\n                seasons.append(item)\n    return seasons\n</code></pre>"},{"location":"api/main/#arai_ai_agents.main.load_agent_config","title":"<code>load_agent_config(agent_name)</code>","text":"<p>Load configuration for the selected agent</p> <p>Parameters:</p> Name Type Description Default <code>agent_name</code> <code>str</code> <p>The name of the agent to load configuration for</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>The configuration for the selected agent</p> Source code in <code>arai_ai_agents/main.py</code> <pre><code>def load_agent_config(agent_name):\n    \"\"\"Load configuration for the selected agent\n\n    Args:\n        agent_name (str): The name of the agent to load configuration for\n\n    Returns:\n        dict: The configuration for the selected agent\n    \"\"\"\n    config_path = os.path.join(\"configs\", agent_name, \"tracker.yaml\")\n    with open(config_path, 'r', encoding='utf-8') as f:\n        return yaml.safe_load(f)\n</code></pre>"},{"location":"api/main/#arai_ai_agents.main.run_scheduler","title":"<code>run_scheduler()</code>","text":"<p>Continuously run the scheduler in a loop </p> Global <p>scheduler_running (bool): Whether the scheduler is running</p> Source code in <code>arai_ai_agents/main.py</code> <pre><code>def run_scheduler():\n    \"\"\" Continuously run the scheduler in a loop \n\n    Global:\n        scheduler_running (bool): Whether the scheduler is running\n    \"\"\"\n    global scheduler_running\n    scheduler_running = True\n    while scheduler_running:\n        schedule.run_pending()\n        time.sleep(1)\n</code></pre>"},{"location":"api/auth/","title":"auth","text":""},{"location":"api/auth/#arai_ai_agents.auth","title":"<code>arai_ai_agents.auth</code>","text":""},{"location":"api/auth/twitter_app_auth/","title":"twitter_app_auth","text":""},{"location":"api/auth/twitter_app_auth/#arai_ai_agents.auth.twitter_app_auth","title":"<code>arai_ai_agents.auth.twitter_app_auth</code>","text":""},{"location":"api/auth/twitter_app_auth/#arai_ai_agents.auth.twitter_app_auth.TwitterAppAuth","title":"<code>TwitterAppAuth</code>","text":"<p>A class for authenticating with the Twitter API using OAuth 1.0a.</p> <p>Attributes:</p> Name Type Description <code>api_key</code> <code>str</code> <p>The API key for the Twitter API.</p> <code>api_secret_key</code> <code>str</code> <p>The API secret key for the Twitter API.</p> <code>bearer_token</code> <code>str</code> <p>The bearer token for the Twitter API.</p> Source code in <code>arai_ai_agents/auth/twitter_app_auth.py</code> <pre><code>class TwitterAppAuth:\n    \"\"\"\n    A class for authenticating with the Twitter API using OAuth 1.0a.\n\n    Attributes:\n        api_key (str): The API key for the Twitter API.\n        api_secret_key (str): The API secret key for the Twitter API.\n        bearer_token (str): The bearer token for the Twitter API.\n    \"\"\"\n    def __init__(self):\n        \"\"\"Initializes the TwitterAppAuth class.\n\n        Example:\n            &gt;&gt;&gt; twitter_auth = TwitterAppAuth()\n        \"\"\"\n        self.api_key = os.getenv(\"TWITTER_API_KEY\") \n        self.api_secret_key = os.getenv(\"TWITTER_API_KEY_SECRET\") \n        self.bearer_token = os.getenv(\"TWITTER_BEARER_TOKEN\") \n\n    def setup_twitter_auth(self):\n        \"\"\"Sets up the Twitter authentication.\n\n        Raises:\n            ValueError: If Twitter API credentials are not present in the environment variables.\n\n        Example:\n            &gt;&gt;&gt; twitter_auth.setup_twitter_auth()\n        \"\"\"\n        try:\n            # Initialize OAuth 1.0a handler\n            self.auth = tweepy.OAuth1UserHandler(\n                consumer_key=self.api_key,\n                consumer_secret=self.api_secret_key,\n                callback=\"oob\"  # Use out-of-band OAuth\n            )\n\n            # Get the authorization URL\n            try:\n                auth_url = self.auth.get_authorization_url()\n                print(\"\\n1. Visit this URL to authorize your app:\")\n                print(auth_url)\n\n                # Open the URL in default browser\n                webbrowser.open(auth_url)\n\n                # Get the verifier code from user\n                print(\"\\n2. Enter the PIN shown on the website:\")\n                verifier = input(\"&gt; \").strip()\n\n                # Get the access tokens\n                try:\n                    access_token, access_token_secret = self.auth.get_access_token(verifier)\n\n                    print(\"\\nSuccess! Add these tokens to your .env file:\")\n                    print(f\"\\nTWITTER_ACCESS_TOKEN={access_token}\")\n                    print(f\"TWITTER_ACCESS_TOKEN_SECRET={access_token_secret}\")\n\n                    # Test the credentials\n                    self.client = tweepy.Client(\n                        bearer_token=self.bearer_token,\n                        consumer_key=self.api_key,\n                        consumer_secret=self.api_secret_key,\n                        access_token=access_token,\n                        access_token_secret=access_token_secret\n                    )\n\n                    me = self.client.get_me()\n                    print(f\"\\nSuccessfully authenticated as @{me.data.username}\")\n\n                    self.save_credentials(access_token, access_token_secret)\n\n                except Exception as e:\n                    print(f\"\\nError getting access tokens: {str(e)}\")\n\n            except Exception as e:\n                print(f\"\\nError getting authorization URL: {str(e)}\")\n                print(\"\\nMake sure your app has OAuth 1.0a enabled in the Twitter Developer Portal:\")\n                print(\"1. Go to https://developer.twitter.com/en/portal/projects\")\n                print(\"2. Select your project and app\")\n                print(\"3. Go to 'User authentication settings'\")\n                print(\"4. Enable 'OAuth 1.0a'\")\n                print(\"5. Set App permissions to 'Read and Write'\")\n                print(\"6. Add 'http://127.0.0.1' to callback URLs\")\n\n        except Exception as e:\n            print(f\"\\nSetup error: {str(e)}\")\n            print(\"\\nCheck that your API credentials are correct in .env file:\")\n            print(\"TWITTER_API_KEY\")\n            print(\"TWITTER_API_KEY_SECRET\")\n\n\n    def save_credentials(self, access_token, access_token_secret):\n        \"\"\"Saves the Twitter access tokens to a YAML file.\n\n        Args:\n            access_token (str): The access token for the Twitter API.\n            access_token_secret (str): The access token secret for the Twitter API.\n\n        Example:\n            &gt;&gt;&gt; twitter_auth.save_credentials(access_token, access_token_secret)\n        \"\"\"\n        try:                  \n            tokens = {\n                'TWITTER_ACCESS_TOKEN': access_token,\n                'TWITTER_ACCESS_TOKEN_SECRET': access_token_secret\n            }\n\n            with open('twitter_tokens.yaml', 'w') as f:\n                yaml.dump(tokens, f)\n\n            print(\"\\nSuccess! Tokens have been saved to twitter_tokens.yaml\")\n\n        except Exception as e:\n            print(f\"\\nError saving tokens: {str(e)}\")\n</code></pre>"},{"location":"api/auth/twitter_app_auth/#arai_ai_agents.auth.twitter_app_auth.TwitterAppAuth.__init__","title":"<code>__init__()</code>","text":"<p>Initializes the TwitterAppAuth class.</p> Example <p>twitter_auth = TwitterAppAuth()</p> Source code in <code>arai_ai_agents/auth/twitter_app_auth.py</code> <pre><code>def __init__(self):\n    \"\"\"Initializes the TwitterAppAuth class.\n\n    Example:\n        &gt;&gt;&gt; twitter_auth = TwitterAppAuth()\n    \"\"\"\n    self.api_key = os.getenv(\"TWITTER_API_KEY\") \n    self.api_secret_key = os.getenv(\"TWITTER_API_KEY_SECRET\") \n    self.bearer_token = os.getenv(\"TWITTER_BEARER_TOKEN\") \n</code></pre>"},{"location":"api/auth/twitter_app_auth/#arai_ai_agents.auth.twitter_app_auth.TwitterAppAuth.save_credentials","title":"<code>save_credentials(access_token, access_token_secret)</code>","text":"<p>Saves the Twitter access tokens to a YAML file.</p> <p>Parameters:</p> Name Type Description Default <code>access_token</code> <code>str</code> <p>The access token for the Twitter API.</p> required <code>access_token_secret</code> <code>str</code> <p>The access token secret for the Twitter API.</p> required Example <p>twitter_auth.save_credentials(access_token, access_token_secret)</p> Source code in <code>arai_ai_agents/auth/twitter_app_auth.py</code> <pre><code>def save_credentials(self, access_token, access_token_secret):\n    \"\"\"Saves the Twitter access tokens to a YAML file.\n\n    Args:\n        access_token (str): The access token for the Twitter API.\n        access_token_secret (str): The access token secret for the Twitter API.\n\n    Example:\n        &gt;&gt;&gt; twitter_auth.save_credentials(access_token, access_token_secret)\n    \"\"\"\n    try:                  \n        tokens = {\n            'TWITTER_ACCESS_TOKEN': access_token,\n            'TWITTER_ACCESS_TOKEN_SECRET': access_token_secret\n        }\n\n        with open('twitter_tokens.yaml', 'w') as f:\n            yaml.dump(tokens, f)\n\n        print(\"\\nSuccess! Tokens have been saved to twitter_tokens.yaml\")\n\n    except Exception as e:\n        print(f\"\\nError saving tokens: {str(e)}\")\n</code></pre>"},{"location":"api/auth/twitter_app_auth/#arai_ai_agents.auth.twitter_app_auth.TwitterAppAuth.setup_twitter_auth","title":"<code>setup_twitter_auth()</code>","text":"<p>Sets up the Twitter authentication.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If Twitter API credentials are not present in the environment variables.</p> Example <p>twitter_auth.setup_twitter_auth()</p> Source code in <code>arai_ai_agents/auth/twitter_app_auth.py</code> <pre><code>def setup_twitter_auth(self):\n    \"\"\"Sets up the Twitter authentication.\n\n    Raises:\n        ValueError: If Twitter API credentials are not present in the environment variables.\n\n    Example:\n        &gt;&gt;&gt; twitter_auth.setup_twitter_auth()\n    \"\"\"\n    try:\n        # Initialize OAuth 1.0a handler\n        self.auth = tweepy.OAuth1UserHandler(\n            consumer_key=self.api_key,\n            consumer_secret=self.api_secret_key,\n            callback=\"oob\"  # Use out-of-band OAuth\n        )\n\n        # Get the authorization URL\n        try:\n            auth_url = self.auth.get_authorization_url()\n            print(\"\\n1. Visit this URL to authorize your app:\")\n            print(auth_url)\n\n            # Open the URL in default browser\n            webbrowser.open(auth_url)\n\n            # Get the verifier code from user\n            print(\"\\n2. Enter the PIN shown on the website:\")\n            verifier = input(\"&gt; \").strip()\n\n            # Get the access tokens\n            try:\n                access_token, access_token_secret = self.auth.get_access_token(verifier)\n\n                print(\"\\nSuccess! Add these tokens to your .env file:\")\n                print(f\"\\nTWITTER_ACCESS_TOKEN={access_token}\")\n                print(f\"TWITTER_ACCESS_TOKEN_SECRET={access_token_secret}\")\n\n                # Test the credentials\n                self.client = tweepy.Client(\n                    bearer_token=self.bearer_token,\n                    consumer_key=self.api_key,\n                    consumer_secret=self.api_secret_key,\n                    access_token=access_token,\n                    access_token_secret=access_token_secret\n                )\n\n                me = self.client.get_me()\n                print(f\"\\nSuccessfully authenticated as @{me.data.username}\")\n\n                self.save_credentials(access_token, access_token_secret)\n\n            except Exception as e:\n                print(f\"\\nError getting access tokens: {str(e)}\")\n\n        except Exception as e:\n            print(f\"\\nError getting authorization URL: {str(e)}\")\n            print(\"\\nMake sure your app has OAuth 1.0a enabled in the Twitter Developer Portal:\")\n            print(\"1. Go to https://developer.twitter.com/en/portal/projects\")\n            print(\"2. Select your project and app\")\n            print(\"3. Go to 'User authentication settings'\")\n            print(\"4. Enable 'OAuth 1.0a'\")\n            print(\"5. Set App permissions to 'Read and Write'\")\n            print(\"6. Add 'http://127.0.0.1' to callback URLs\")\n\n    except Exception as e:\n        print(f\"\\nSetup error: {str(e)}\")\n        print(\"\\nCheck that your API credentials are correct in .env file:\")\n        print(\"TWITTER_API_KEY\")\n        print(\"TWITTER_API_KEY_SECRET\")\n</code></pre>"},{"location":"api/connectors/","title":"connectors","text":""},{"location":"api/connectors/#arai_ai_agents.connectors","title":"<code>arai_ai_agents.connectors</code>","text":""},{"location":"api/connectors/twitter_connector/","title":"twitter_connector","text":""},{"location":"api/connectors/twitter_connector/#arai_ai_agents.connectors.twitter_connector","title":"<code>arai_ai_agents.connectors.twitter_connector</code>","text":""},{"location":"api/connectors/twitter_connector/#arai_ai_agents.connectors.twitter_connector.TwitterConnector","title":"<code>TwitterConnector</code>","text":"<p>A connector for interacting with the Twitter API via Tweepy.</p> <p>This class handles OAuth 1.0a and OAuth 2.0 authentication for posting tweets and retrieving data from Twitter.</p> <p>Attributes:</p> Name Type Description <code>api_key</code> <code>str</code> <p>The API key for the Twitter API.</p> <code>api_secret_key</code> <code>str</code> <p>The API secret key for the Twitter API.</p> <code>access_token</code> <code>str</code> <p>The access token for the Twitter API.</p> <code>access_token_secret</code> <code>str</code> <p>The access token secret for the Twitter API.</p> <code>bearer_token</code> <code>str</code> <p>The bearer token for the Twitter API.</p> <code>bot_username</code> <code>str</code> <p>The username of the bot.</p> <code>bot_id</code> <code>str</code> <p>The ID of the bot.</p> Source code in <code>arai_ai_agents/connectors/twitter_connector.py</code> <pre><code>class TwitterConnector:\n    \"\"\"A connector for interacting with the Twitter API via Tweepy.\n\n    This class handles OAuth 1.0a and OAuth 2.0 authentication for\n    posting tweets and retrieving data from Twitter.\n\n    Attributes:\n        api_key (str): The API key for the Twitter API.\n        api_secret_key (str): The API secret key for the Twitter API.\n        access_token (str): The access token for the Twitter API.\n        access_token_secret (str): The access token secret for the Twitter API.\n        bearer_token (str): The bearer token for the Twitter API.\n        bot_username (str): The username of the bot.\n        bot_id (str): The ID of the bot.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"Initializes the Twitter connector.\n\n        Raises:\n            ValueError: If Twitter API credentials are not present in the environment variables.\n        \"\"\"\n\n        self.api_key = os.getenv(\"TWITTER_API_KEY\") \n        # Part of the OAuth 1.0a credentials identifying the application (required for user-based authentication).\n\n        self.api_secret_key = os.getenv(\"TWITTER_API_KEY_SECRET\") \n        # Secret counterpart to the API key, used in signing OAuth 1.0a requests.\n\n        self.access_token = os.getenv(\"TWITTER_ACCESS_TOKEN\") \n        # Represents the user\u2019s OAuth 1.0a credentials, required for user-level actions (e.g., posting tweets).\n\n        self.access_token_secret = os.getenv(\"TWITTER_ACCESS_TOKEN_SECRET\") \n        # Secret counterpart to the access token, used in signing user-level requests under OAuth 1.0a.\n\n        self.bearer_token = os.getenv(\"TWITTER_BEARER_TOKEN\") \n        # Used for OAuth 2.0 app-only authentication in Twitter API v2, often for read-only access to public data.\n\n        if not all([self.api_key, self.api_secret_key, self.access_token, self.access_token_secret]):\n            raise ValueError(\"Twitter API credentials are not set in .env file\")\n\n        try:\n            # Initialize v2 client only\n            self.client = tweepy.Client(\n                bearer_token=self.bearer_token,\n                consumer_key=self.api_key,\n                consumer_secret=self.api_secret_key,\n                access_token=self.access_token,\n                access_token_secret=self.access_token_secret,\n                wait_on_rate_limit=True\n            )\n\n            # Get the bot's info using v2 API\n            me = self.client.get_me()\n            self.bot_username = me.data.username.lower()\n            self.bot_id = me.data.id\n            print(f\"Successfully authenticated as @{self.bot_username}\")\n\n            self.last_mention_id = None\n        except Exception as e:\n            print(f\"Twitter authentication failed: {str(e)}\")\n            raise\n\n    def post_tweet(self, message: str) -&gt; str:\n        \"\"\"Posts a tweet to the bot's Twitter account.\n\n        Args:\n            message (str): The message to be posted as a tweet. Must be shorter than 280 characters.\n\n        Returns:\n            str: A string message indicating success or failure.\n\n        Raises:\n            Exception: If there's an error posting the tweet.\n\n        Example:\n            &gt;&gt;&gt; twitter_connector.post_tweet(\"Hello, world!\")\n        \"\"\"\n        try:\n            if not message:                \n                return \"Error: Tweet message is empty\"\n\n            # Truncate if too long\n            if len(message) &gt; 280:\n                message = message[:277] + \"...\"\n\n            self.client.create_tweet(text=message)            \n            return \"Tweeted: \" + message\n        except Exception as e:            \n            return \"Error posting tweet: \" + str(e)\n</code></pre>"},{"location":"api/connectors/twitter_connector/#arai_ai_agents.connectors.twitter_connector.TwitterConnector.__init__","title":"<code>__init__()</code>","text":"<p>Initializes the Twitter connector.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If Twitter API credentials are not present in the environment variables.</p> Source code in <code>arai_ai_agents/connectors/twitter_connector.py</code> <pre><code>def __init__(self):\n    \"\"\"Initializes the Twitter connector.\n\n    Raises:\n        ValueError: If Twitter API credentials are not present in the environment variables.\n    \"\"\"\n\n    self.api_key = os.getenv(\"TWITTER_API_KEY\") \n    # Part of the OAuth 1.0a credentials identifying the application (required for user-based authentication).\n\n    self.api_secret_key = os.getenv(\"TWITTER_API_KEY_SECRET\") \n    # Secret counterpart to the API key, used in signing OAuth 1.0a requests.\n\n    self.access_token = os.getenv(\"TWITTER_ACCESS_TOKEN\") \n    # Represents the user\u2019s OAuth 1.0a credentials, required for user-level actions (e.g., posting tweets).\n\n    self.access_token_secret = os.getenv(\"TWITTER_ACCESS_TOKEN_SECRET\") \n    # Secret counterpart to the access token, used in signing user-level requests under OAuth 1.0a.\n\n    self.bearer_token = os.getenv(\"TWITTER_BEARER_TOKEN\") \n    # Used for OAuth 2.0 app-only authentication in Twitter API v2, often for read-only access to public data.\n\n    if not all([self.api_key, self.api_secret_key, self.access_token, self.access_token_secret]):\n        raise ValueError(\"Twitter API credentials are not set in .env file\")\n\n    try:\n        # Initialize v2 client only\n        self.client = tweepy.Client(\n            bearer_token=self.bearer_token,\n            consumer_key=self.api_key,\n            consumer_secret=self.api_secret_key,\n            access_token=self.access_token,\n            access_token_secret=self.access_token_secret,\n            wait_on_rate_limit=True\n        )\n\n        # Get the bot's info using v2 API\n        me = self.client.get_me()\n        self.bot_username = me.data.username.lower()\n        self.bot_id = me.data.id\n        print(f\"Successfully authenticated as @{self.bot_username}\")\n\n        self.last_mention_id = None\n    except Exception as e:\n        print(f\"Twitter authentication failed: {str(e)}\")\n        raise\n</code></pre>"},{"location":"api/connectors/twitter_connector/#arai_ai_agents.connectors.twitter_connector.TwitterConnector.post_tweet","title":"<code>post_tweet(message)</code>","text":"<p>Posts a tweet to the bot's Twitter account.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>The message to be posted as a tweet. Must be shorter than 280 characters.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A string message indicating success or failure.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If there's an error posting the tweet.</p> Example <p>twitter_connector.post_tweet(\"Hello, world!\")</p> Source code in <code>arai_ai_agents/connectors/twitter_connector.py</code> <pre><code>def post_tweet(self, message: str) -&gt; str:\n    \"\"\"Posts a tweet to the bot's Twitter account.\n\n    Args:\n        message (str): The message to be posted as a tweet. Must be shorter than 280 characters.\n\n    Returns:\n        str: A string message indicating success or failure.\n\n    Raises:\n        Exception: If there's an error posting the tweet.\n\n    Example:\n        &gt;&gt;&gt; twitter_connector.post_tweet(\"Hello, world!\")\n    \"\"\"\n    try:\n        if not message:                \n            return \"Error: Tweet message is empty\"\n\n        # Truncate if too long\n        if len(message) &gt; 280:\n            message = message[:277] + \"...\"\n\n        self.client.create_tweet(text=message)            \n        return \"Tweeted: \" + message\n    except Exception as e:            \n        return \"Error posting tweet: \" + str(e)\n</code></pre>"},{"location":"api/models/","title":"models","text":""},{"location":"api/models/#arai_ai_agents.models","title":"<code>arai_ai_agents.models</code>","text":""},{"location":"api/models/base_model/","title":"base_model","text":""},{"location":"api/models/base_model/#arai_ai_agents.models.base_model","title":"<code>arai_ai_agents.models.base_model</code>","text":""},{"location":"api/models/base_model/#arai_ai_agents.models.base_model.ModelInterface","title":"<code>ModelInterface</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for all models.</p> Source code in <code>arai_ai_agents/models/base_model.py</code> <pre><code>class ModelInterface(ABC):\n    \"\"\"Base class for all models.        \n    \"\"\"\n    @abstractmethod\n    def generate_response(self, prompt: str) -&gt; str:\n        \"\"\" Generate a response to a given prompt.\n\n        Args:\n            prompt (str): The prompt to generate a response to.\n\n        Returns:\n            str: The generated response.\n\n        Example:\n            &gt;&gt;&gt; base_model.generate_response(\"Hello, world!\")\n        \"\"\"\n\n        pass\n</code></pre>"},{"location":"api/models/base_model/#arai_ai_agents.models.base_model.ModelInterface.generate_response","title":"<code>generate_response(prompt)</code>  <code>abstractmethod</code>","text":"<p>Generate a response to a given prompt.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>The prompt to generate a response to.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The generated response.</p> Example <p>base_model.generate_response(\"Hello, world!\")</p> Source code in <code>arai_ai_agents/models/base_model.py</code> <pre><code>@abstractmethod\ndef generate_response(self, prompt: str) -&gt; str:\n    \"\"\" Generate a response to a given prompt.\n\n    Args:\n        prompt (str): The prompt to generate a response to.\n\n    Returns:\n        str: The generated response.\n\n    Example:\n        &gt;&gt;&gt; base_model.generate_response(\"Hello, world!\")\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"api/models/gemini_model/","title":"gemini_model","text":""},{"location":"api/models/gemini_model/#arai_ai_agents.models.gemini_model","title":"<code>arai_ai_agents.models.gemini_model</code>","text":""},{"location":"api/models/gemini_model/#arai_ai_agents.models.gemini_model.GeminiModel","title":"<code>GeminiModel</code>","text":"<p>               Bases: <code>ModelInterface</code></p> <p>Gemini model implementation.</p> <p>Attributes:</p> Name Type Description <code>model</code> <code>str</code> <p>The name of the Gemini model to use.</p> Source code in <code>arai_ai_agents/models/gemini_model.py</code> <pre><code>class GeminiModel(ModelInterface):\n    \"\"\"Gemini model implementation.\n\n    Attributes:\n        model (str): The name of the Gemini model to use.\n    \"\"\"\n\n    def __init__(self, api_key=None, model_name=\"gemini-exp-1206\"):\n        \"\"\"Initialize the Gemini model.\n\n        Args:\n            api_key (str): The API key to use for the Gemini model.\n            model_name (str): The name of the Gemini model to use.\n\n        Example:\n            &gt;&gt;&gt; gemini_model = GeminiModel()\n        \"\"\"\n        if api_key:\n            genai.configure(api_key=api_key)\n        else:\n            genai.configure(api_key=os.environ.get('GOOGLE_GEMINI_API_KEY'))\n        self.model = genai.GenerativeModel(model_name)\n\n    # -------------------------------------------------------------------\n    # Helper to generate a response to a given prompt using the Gemini API.\n    # -------------------------------------------------------------------\n    def generate_response(self, prompt, **kwargs):\n        \"\"\"Generate a response to a given prompt using the Gemini API.\n\n        Args:\n            prompt (str): The prompt to generate a response to.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            str: The generated response.\n\n        Example:\n            &gt;&gt;&gt; gemini_model = GeminiModel()\n            &gt;&gt;&gt; response = gemini_model.generate_response(\"What is the weather in Tokyo?\")\n        \"\"\"\n        if isinstance(prompt, str):\n            return self.generate_response_from_string(prompt, **kwargs)       \n        elif isinstance(prompt, list[dict]):\n            return self.generate_response_dictionary(prompt)\n\n    # -------------------------------------------------------------------\n    # Helper to generate a response to a given prompt using a list of dictionaries\n    # -------------------------------------------------------------------\n    def generate_response_dictionary(self, prompt: list[dict]) -&gt; str:\n        \"\"\"Generate a response to a given prompt using a list of dictionaries.\n\n        Args:\n            prompt (list[dict]): The prompt to generate a response to.\n\n        Returns:\n            str: The generated response.\n\n        Example:\n            &gt;&gt;&gt; gemini_model = GeminiModel()\n            &gt;&gt;&gt; response = gemini_model.generate_response_dictionary([{\"role\": \"user\", \"parts\": \"What is the weather in Tokyo?\"}])\n        \"\"\"\n        try:\n            response = self.model.generate_content(prompt)\n            return response.text.strip()\n        except Exception as e:\n            return f\"Error generating response: {str(e)}\"\n\n    # -------------------------------------------------------------------\n    # Helper to generate a response to a given prompt using a string\n    # -------------------------------------------------------------------\n    def generate_response_from_string(self, prompt, **kwargs):\n        \"\"\"\n        Description: \n            Generate a response to a given prompt using a string.\n\n        Args:\n            prompt (str): The prompt to generate a response to.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            str: The generated response.\n\n        Example:\n            &gt;&gt;&gt; gemini_model = GeminiModel()\n            &gt;&gt;&gt; response = gemini_model.generate_response_from_string(\"What is the weather in Tokyo?\")\n        \"\"\"\n        # Extract personality and style from kwargs, or use defaults from agent_template\n        if kwargs:\n            if \"personality\" in kwargs:\n                personality = kwargs.get(\"personality\")\n            if \"communication_style\" in kwargs:\n                communication_style = kwargs.get(\"communication_style\")\n        else:\n            personality = \"\"\n            communication_style = \"\"        \n\n        try:\n            # instructions being sent to the ai model\n            messages = []\n\n            # add personality and style to the instructions\n            if personality or communication_style:\n                persona_prompt = f\"{personality} {communication_style}\"\n                messages.append({\n                    \"role\": \"user\",\n                    \"parts\": [persona_prompt]\n                })\n\n            # user message\n            messages.append({\n                \"role\": \"user\",\n                \"parts\": [prompt]\n            })\n\n            # Make sure that what is being sent to the model is correct\n            # print(messages)\n\n            # generate the response\n            response = self.model.generate_content(messages)\n            return response.text.strip()\n\n        except Exception as e:\n            return f\"Error generating response: {str(e)}\"\n\n    # -------------------------------------------------------------------\n    # Helper to fix a response that is not valid YAML\n    # -------------------------------------------------------------------\n    def fix_response(self, prompt, response):\n        \"\"\"Fix a response that is not valid YAML.\n\n        Args:\n            prompt (str): The prompt to generate a response to.\n            response (str): The response to fix.\n\n        Returns:\n            str: The fixed response.\n\n        Raises:\n            Exception: If there's an error calling the API.\n\n        Example:\n            &gt;&gt;&gt; gemini_model = GeminiModel()\n            &gt;&gt;&gt; response = gemini_model.fix_response(\"What is the weather in Tokyo?\", \"The weather in Tokyo is sunny.\")\n        \"\"\"\n        try:\n            # instructions being sent to the ai model\n            messages = []\n\n            # add personality and style to the instructions            \n            messages.append({\n                \"role\": \"user\",\n                \"parts\": [prompt]\n            })\n\n            # user message\n            messages.append({\n                \"role\": \"user\",\n                \"parts\": [response]\n            })\n\n            # Make sure that what is being sent to the model is correct\n            # print(messages)\n\n            # generate the response\n            response = self.model.generate_content(messages)\n            return response.text.strip()\n\n        except Exception as e:\n            return f\"Error generating response: {str(e)}\"\n\n    def generate_yaml_response(self):\n        \"\"\"Generate a YAML response.\n\n        Args:\n            None\n\n        Returns:\n            None\n\n        Example:\n            &gt;&gt;&gt; gemini_model = GeminiModel()\n            &gt;&gt;&gt; gemini_model.generate_yaml_response()\n        \"\"\"\n        messages = []\n\n        messages.append({\n            \"role\": \"user\",\n            \"parts\": \"Create me a YAML file with the following fields: name, personality, communication_style, topic, backstory, universe, hashtags, emojis. Start and end the file with ```yaml and ```\"\n        })\n\n        response = self.model.generate_content(messages)\n        print(response)\n\n        # save the response to a file\n        with open(\"response.txt\", \"w\", encoding=\"utf-8\") as f:\n            f.write(response.text.strip())\n\n        # save the response to a file\n        with open(\"response.yaml\", \"w\", encoding=\"utf-8\") as f:\n            f.write(response.text)\n\n        # strip the response\n        with open(\"response_stripped.yaml\", \"w\", encoding=\"utf-8\") as f:\n            f.write(response.text.strip())\n\n\n        # save the response to a yaml file\n        with open(\"yaml_response.yaml\", \"w\", encoding=\"utf-8\") as f:\n            yaml.dump(response.text, f)       \n\n        # strip the response\n        with open(\"yaml_response_stripped.yaml\", \"w\", encoding=\"utf-8\") as f:\n            yaml.dump(response.text.strip(), f)    \n</code></pre>"},{"location":"api/models/gemini_model/#arai_ai_agents.models.gemini_model.GeminiModel.__init__","title":"<code>__init__(api_key=None, model_name='gemini-exp-1206')</code>","text":"<p>Initialize the Gemini model.</p> <p>Parameters:</p> Name Type Description Default <code>api_key</code> <code>str</code> <p>The API key to use for the Gemini model.</p> <code>None</code> <code>model_name</code> <code>str</code> <p>The name of the Gemini model to use.</p> <code>'gemini-exp-1206'</code> Example <p>gemini_model = GeminiModel()</p> Source code in <code>arai_ai_agents/models/gemini_model.py</code> <pre><code>def __init__(self, api_key=None, model_name=\"gemini-exp-1206\"):\n    \"\"\"Initialize the Gemini model.\n\n    Args:\n        api_key (str): The API key to use for the Gemini model.\n        model_name (str): The name of the Gemini model to use.\n\n    Example:\n        &gt;&gt;&gt; gemini_model = GeminiModel()\n    \"\"\"\n    if api_key:\n        genai.configure(api_key=api_key)\n    else:\n        genai.configure(api_key=os.environ.get('GOOGLE_GEMINI_API_KEY'))\n    self.model = genai.GenerativeModel(model_name)\n</code></pre>"},{"location":"api/models/gemini_model/#arai_ai_agents.models.gemini_model.GeminiModel.fix_response","title":"<code>fix_response(prompt, response)</code>","text":"<p>Fix a response that is not valid YAML.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>The prompt to generate a response to.</p> required <code>response</code> <code>str</code> <p>The response to fix.</p> required <p>Returns:</p> Name Type Description <code>str</code> <p>The fixed response.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If there's an error calling the API.</p> Example <p>gemini_model = GeminiModel() response = gemini_model.fix_response(\"What is the weather in Tokyo?\", \"The weather in Tokyo is sunny.\")</p> Source code in <code>arai_ai_agents/models/gemini_model.py</code> <pre><code>def fix_response(self, prompt, response):\n    \"\"\"Fix a response that is not valid YAML.\n\n    Args:\n        prompt (str): The prompt to generate a response to.\n        response (str): The response to fix.\n\n    Returns:\n        str: The fixed response.\n\n    Raises:\n        Exception: If there's an error calling the API.\n\n    Example:\n        &gt;&gt;&gt; gemini_model = GeminiModel()\n        &gt;&gt;&gt; response = gemini_model.fix_response(\"What is the weather in Tokyo?\", \"The weather in Tokyo is sunny.\")\n    \"\"\"\n    try:\n        # instructions being sent to the ai model\n        messages = []\n\n        # add personality and style to the instructions            \n        messages.append({\n            \"role\": \"user\",\n            \"parts\": [prompt]\n        })\n\n        # user message\n        messages.append({\n            \"role\": \"user\",\n            \"parts\": [response]\n        })\n\n        # Make sure that what is being sent to the model is correct\n        # print(messages)\n\n        # generate the response\n        response = self.model.generate_content(messages)\n        return response.text.strip()\n\n    except Exception as e:\n        return f\"Error generating response: {str(e)}\"\n</code></pre>"},{"location":"api/models/gemini_model/#arai_ai_agents.models.gemini_model.GeminiModel.generate_response","title":"<code>generate_response(prompt, **kwargs)</code>","text":"<p>Generate a response to a given prompt using the Gemini API.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>The prompt to generate a response to.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>str</code> <p>The generated response.</p> Example <p>gemini_model = GeminiModel() response = gemini_model.generate_response(\"What is the weather in Tokyo?\")</p> Source code in <code>arai_ai_agents/models/gemini_model.py</code> <pre><code>def generate_response(self, prompt, **kwargs):\n    \"\"\"Generate a response to a given prompt using the Gemini API.\n\n    Args:\n        prompt (str): The prompt to generate a response to.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        str: The generated response.\n\n    Example:\n        &gt;&gt;&gt; gemini_model = GeminiModel()\n        &gt;&gt;&gt; response = gemini_model.generate_response(\"What is the weather in Tokyo?\")\n    \"\"\"\n    if isinstance(prompt, str):\n        return self.generate_response_from_string(prompt, **kwargs)       \n    elif isinstance(prompt, list[dict]):\n        return self.generate_response_dictionary(prompt)\n</code></pre>"},{"location":"api/models/gemini_model/#arai_ai_agents.models.gemini_model.GeminiModel.generate_response_dictionary","title":"<code>generate_response_dictionary(prompt)</code>","text":"<p>Generate a response to a given prompt using a list of dictionaries.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>list[dict]</code> <p>The prompt to generate a response to.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The generated response.</p> Example <p>gemini_model = GeminiModel() response = gemini_model.generate_response_dictionary([{\"role\": \"user\", \"parts\": \"What is the weather in Tokyo?\"}])</p> Source code in <code>arai_ai_agents/models/gemini_model.py</code> <pre><code>def generate_response_dictionary(self, prompt: list[dict]) -&gt; str:\n    \"\"\"Generate a response to a given prompt using a list of dictionaries.\n\n    Args:\n        prompt (list[dict]): The prompt to generate a response to.\n\n    Returns:\n        str: The generated response.\n\n    Example:\n        &gt;&gt;&gt; gemini_model = GeminiModel()\n        &gt;&gt;&gt; response = gemini_model.generate_response_dictionary([{\"role\": \"user\", \"parts\": \"What is the weather in Tokyo?\"}])\n    \"\"\"\n    try:\n        response = self.model.generate_content(prompt)\n        return response.text.strip()\n    except Exception as e:\n        return f\"Error generating response: {str(e)}\"\n</code></pre>"},{"location":"api/models/gemini_model/#arai_ai_agents.models.gemini_model.GeminiModel.generate_response_from_string","title":"<code>generate_response_from_string(prompt, **kwargs)</code>","text":"Description <p>Generate a response to a given prompt using a string.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>The prompt to generate a response to.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>str</code> <p>The generated response.</p> Example <p>gemini_model = GeminiModel() response = gemini_model.generate_response_from_string(\"What is the weather in Tokyo?\")</p> Source code in <code>arai_ai_agents/models/gemini_model.py</code> <pre><code>def generate_response_from_string(self, prompt, **kwargs):\n    \"\"\"\n    Description: \n        Generate a response to a given prompt using a string.\n\n    Args:\n        prompt (str): The prompt to generate a response to.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        str: The generated response.\n\n    Example:\n        &gt;&gt;&gt; gemini_model = GeminiModel()\n        &gt;&gt;&gt; response = gemini_model.generate_response_from_string(\"What is the weather in Tokyo?\")\n    \"\"\"\n    # Extract personality and style from kwargs, or use defaults from agent_template\n    if kwargs:\n        if \"personality\" in kwargs:\n            personality = kwargs.get(\"personality\")\n        if \"communication_style\" in kwargs:\n            communication_style = kwargs.get(\"communication_style\")\n    else:\n        personality = \"\"\n        communication_style = \"\"        \n\n    try:\n        # instructions being sent to the ai model\n        messages = []\n\n        # add personality and style to the instructions\n        if personality or communication_style:\n            persona_prompt = f\"{personality} {communication_style}\"\n            messages.append({\n                \"role\": \"user\",\n                \"parts\": [persona_prompt]\n            })\n\n        # user message\n        messages.append({\n            \"role\": \"user\",\n            \"parts\": [prompt]\n        })\n\n        # Make sure that what is being sent to the model is correct\n        # print(messages)\n\n        # generate the response\n        response = self.model.generate_content(messages)\n        return response.text.strip()\n\n    except Exception as e:\n        return f\"Error generating response: {str(e)}\"\n</code></pre>"},{"location":"api/models/gemini_model/#arai_ai_agents.models.gemini_model.GeminiModel.generate_yaml_response","title":"<code>generate_yaml_response()</code>","text":"<p>Generate a YAML response.</p> <p>Returns:</p> Type Description <p>None</p> Example <p>gemini_model = GeminiModel() gemini_model.generate_yaml_response()</p> Source code in <code>arai_ai_agents/models/gemini_model.py</code> <pre><code>def generate_yaml_response(self):\n    \"\"\"Generate a YAML response.\n\n    Args:\n        None\n\n    Returns:\n        None\n\n    Example:\n        &gt;&gt;&gt; gemini_model = GeminiModel()\n        &gt;&gt;&gt; gemini_model.generate_yaml_response()\n    \"\"\"\n    messages = []\n\n    messages.append({\n        \"role\": \"user\",\n        \"parts\": \"Create me a YAML file with the following fields: name, personality, communication_style, topic, backstory, universe, hashtags, emojis. Start and end the file with ```yaml and ```\"\n    })\n\n    response = self.model.generate_content(messages)\n    print(response)\n\n    # save the response to a file\n    with open(\"response.txt\", \"w\", encoding=\"utf-8\") as f:\n        f.write(response.text.strip())\n\n    # save the response to a file\n    with open(\"response.yaml\", \"w\", encoding=\"utf-8\") as f:\n        f.write(response.text)\n\n    # strip the response\n    with open(\"response_stripped.yaml\", \"w\", encoding=\"utf-8\") as f:\n        f.write(response.text.strip())\n\n\n    # save the response to a yaml file\n    with open(\"yaml_response.yaml\", \"w\", encoding=\"utf-8\") as f:\n        yaml.dump(response.text, f)       \n\n    # strip the response\n    with open(\"yaml_response_stripped.yaml\", \"w\", encoding=\"utf-8\") as f:\n        yaml.dump(response.text.strip(), f)    \n</code></pre>"},{"location":"api/prompt_chaining/","title":"prompt_chaining","text":""},{"location":"api/prompt_chaining/#arai_ai_agents.prompt_chaining","title":"<code>arai_ai_agents.prompt_chaining</code>","text":""},{"location":"api/prompt_chaining/step_1/","title":"step_1","text":""},{"location":"api/prompt_chaining/step_1/#arai_ai_agents.prompt_chaining.step_1","title":"<code>arai_ai_agents.prompt_chaining.step_1</code>","text":""},{"location":"api/prompt_chaining/step_1/#arai_ai_agents.prompt_chaining.step_1.step_1","title":"<code>step_1(ai_model, concept)</code>","text":"Description <p>Create a new agent</p> <p>Parameters:</p> Name Type Description Default <code>ai_model</code> <p>The AI model to use for generating responses</p> required <code>debug</code> <code>bool</code> <p>whether to print debug information. Defaults to False.</p> required <p>Returns:</p> Name Type Description <code>agent_file_path</code> <p>The path to the agent yaml file</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If there's an error creating the agent</p> <code>Example</code> Source code in <code>arai_ai_agents/prompt_chaining/step_1.py</code> <pre><code>def step_1(ai_model, concept: str):\n    '''\n    Description:\n        Create a new agent\n\n    Args:\n        ai_model: The AI model to use for generating responses\n        debug (bool, optional): whether to print debug information. Defaults to False.\n\n    Returns:\n        agent_file_path: The path to the agent yaml file\n\n    Raises:\n        Exception: If there's an error creating the agent\n\n        Example:\n        &gt;&gt;&gt; ai_model = GeminiModel()\n        &gt;&gt;&gt; step_1(ai_model, \"alien drone pilot who is a sarcastic asshole visiting earth to report back his findings to his home planet\")\n    '''\n\n    # Step 1.1: Create a new agent\n    manager = ContentGenerator()\n    agent_template = manager.create_new_template_yaml(TemplateType.AGENT)\n\n    # step 1.2: Generate a new agent name, topic, personality, and communication style with the prompt_1 template\n    # prompt 1 Character Creation:\n    prompt_1_vars = {\n        # \"agent_name\": \"\",\n        # \"personality\": \"\",\n        # \"communication_style\": \"\",\n        # \"topic\": \"\",\n        # \"concept\": \"alien drone pilot who is a sarcastic asshole visiting earth to report back his findings to his home planet\",\n        \"concept\": concept,\n        \"agent_yaml\": yaml.dump(agent_template)        \n    }\n\n    # step 1.3: Run the prompt\n    agent_data = manager.run_prompt(\n        # prompt_key=\"prompt_1 (Character Creation)\",\n        prompt_key=\"prompt_1 (Character Sheet Creation)\",\n        template_vars=prompt_1_vars, \n        ai_model=ai_model\n    )\n\n    # step 1.4: Add the agent data to the agent template\n    agent_template = manager.add_data_to_template(\n        current_data=agent_template,\n        new_data=agent_data\n    )\n\n    # step 1.5: store the concept in the agent template\n    agent_template[\"concept\"] = prompt_1_vars[\"concept\"]\n\n    # step 1.6: create the file path\n    agent_file_path = manager.create_filepath(\n        agent_name=agent_template[\"name\"], \n        season_number=0,\n        episode_number=0,\n        template_type=TemplateType.AGENT\n    )\n\n    # step 1.7: Save the agent data to a file\n    manager.save_yaml_file(\n        save_path=agent_file_path,\n        yaml_data=agent_template\n    )\n\n    return agent_file_path\n</code></pre>"},{"location":"api/prompt_chaining/step_2/","title":"step_2","text":""},{"location":"api/prompt_chaining/step_2/#arai_ai_agents.prompt_chaining.step_2","title":"<code>arai_ai_agents.prompt_chaining.step_2</code>","text":""},{"location":"api/prompt_chaining/step_2/#arai_ai_agents.prompt_chaining.step_2.step_2","title":"<code>step_2(ai_model, agent_file_path)</code>","text":"Description <p>Create a new season for the agent</p> <p>Parameters:</p> Name Type Description Default <code>ai_model</code> <p>The AI model to use for generating responses</p> required <code>agent_file_path</code> <p>The path to the agent yaml file</p> required <p>Returns:</p> Name Type Description <code>season_file_path</code> <p>The path to the season yaml file</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If there's an error creating the season</p> Example <p>ai_model = GeminiModel() agent_file_path = \"agent_template.yaml\" step_2(ai_model, agent_file_path)</p> Source code in <code>arai_ai_agents/prompt_chaining/step_2.py</code> <pre><code>def step_2(ai_model, agent_file_path):\n    '''\n    Description:\n        Create a new season for the agent\n\n    Args:\n        ai_model: The AI model to use for generating responses\n        agent_file_path: The path to the agent yaml file\n\n    Returns:\n        season_file_path: The path to the season yaml file\n\n    Raises:\n        Exception: If there's an error creating the season\n\n    Example:\n        &gt;&gt;&gt; ai_model = GeminiModel()\n        &gt;&gt;&gt; agent_file_path = \"agent_template.yaml\"\n        &gt;&gt;&gt; step_2(ai_model, agent_file_path)\n    '''\n    print(\"Step 2: Create a new season\") \n\n    # step 2.1: load the season template yaml file\n    manager = ContentGenerator()\n    season_template = manager.create_new_template_yaml(TemplateType.SEASON)\n\n    # step 2.2: load the agent yaml file\n    agent_yaml = None    \n    with open(agent_file_path, 'r', encoding='utf-8') as file:\n        agent_yaml = yaml.safe_load(file)\n\n    # step 2.3: find the previous season \n    # get last season file \n    previous_season = None\n    # Update the path to include the agent's specific directory\n    agent_seasons_dir = os.path.join(manager.agents_config_dir, agent_yaml['name'])\n\n\n    # Check if the agent directory exists\n    if os.path.exists(agent_seasons_dir):\n        # Look for season folders in the agent's directory\n        season_folders = [f for f in os.listdir(agent_seasons_dir) if f.startswith(\"season_\")]\n\n        if season_folders:\n            # Sort the folders by season number and get the last one\n            last_folder = sorted(season_folders, key=lambda x: int(x.split('_')[1]))[-1]\n\n            season_file_path = os.path.join(agent_seasons_dir, last_folder, f\"{last_folder}.yaml\")\n            print(f\"Looking for previous season at: {season_file_path}\")  # Debug print\n\n            if os.path.exists(season_file_path):\n                with open(season_file_path, 'r', encoding='utf-8') as file:\n                    previous_season = yaml.safe_load(file)\n                print(f\"Found previous season: {last_folder}\")\n        else:\n            print(f\"No season folders found in {agent_seasons_dir}\")\n    else:\n        print(f\"Agent directory not found at {agent_seasons_dir}\")\n\n\n\n    # step 2.3: Generate a new season name, topic, and communication style with the prompt_2 template\n    # prompt 2 Season Creation:\n    # note that emojis will be output as unicode characters due to the yaml dump\n    prompt_2_vars = {\n        \"agent_name\": agent_yaml[\"name\"],\n        \"agent_yaml\": yaml.dump(agent_yaml),\n        \"season_yaml\": yaml.dump(season_template),\n        \"previous_season\": yaml.dump(previous_season)\n    }\n\n    # step 2.4: Run the prompt \n    season_data = manager.run_prompt(\n        # prompt_key=\"prompt_1 (Character Creation)\",\n        prompt_key=\"prompt_2 (Season Creation)\",\n        template_vars=prompt_2_vars, \n        ai_model=ai_model,\n    )\n\n    # step 2.5: Add the season data to the season template\n    season_template = manager.add_data_to_template(\n        current_data=season_template,\n        new_data=season_data, \n    )\n\n    # step 2.6: create the file path\n    season_file_path = manager.create_filepath(\n        agent_name=agent_yaml[\"name\"],\n        season_number=season_template[\"season\"][\"season_number\"],\n        episode_number=0,\n        template_type=TemplateType.SEASON\n    )\n\n    # step 2.7: Save the season data to a file\n    manager.save_yaml_file(\n        save_path=season_file_path,\n        yaml_data=season_template\n    )\n\n    return season_file_path\n</code></pre>"},{"location":"api/prompt_chaining/step_3/","title":"step_3","text":""},{"location":"api/prompt_chaining/step_3/#arai_ai_agents.prompt_chaining.step_3","title":"<code>arai_ai_agents.prompt_chaining.step_3</code>","text":""},{"location":"api/prompt_chaining/step_3/#arai_ai_agents.prompt_chaining.step_3.step_3","title":"<code>step_3(ai_model, agent_file_path, season_file_path)</code>","text":"Description <p>Create a new episode posts for the agent</p> <p>Parameters:</p> Name Type Description Default <code>ai_model</code> <p>The AI model to use for generating responses</p> required <code>agent_file_path</code> <p>The path to the agent yaml file</p> required <code>season_file_path</code> <p>The path to the season yaml file</p> required <p>Raises:</p> Type Description <code>Exception</code> <p>If there's an error creating the episode posts        </p> Example <p>ai_model = GeminiModel() agent_file_path = \"agent_template.yaml\" season_file_path = \"season_template.yaml\" step_3(ai_model, agent_file_path, season_file_path)</p> Source code in <code>arai_ai_agents/prompt_chaining/step_3.py</code> <pre><code>def step_3(ai_model, agent_file_path, season_file_path):\n    '''\n    Description:\n        Create a new episode posts for the agent\n\n    Args:\n        ai_model: The AI model to use for generating responses\n        agent_file_path: The path to the agent yaml file\n        season_file_path: The path to the season yaml file\n\n    Raises:\n        Exception: If there's an error creating the episode posts        \n\n    Example:\n        &gt;&gt;&gt; ai_model = GeminiModel()\n        &gt;&gt;&gt; agent_file_path = \"agent_template.yaml\"\n        &gt;&gt;&gt; season_file_path = \"season_template.yaml\"\n        &gt;&gt;&gt; step_3(ai_model, agent_file_path, season_file_path)\n    '''\n    print(\"Step 3: Create a batch of posts for episode\") \n\n    # step 3.1: load the season template yaml file\n    manager = ContentGenerator()\n\n    # step 3.2: load the agent yaml file\n    agent_yaml = None    \n    with open(agent_file_path, 'r', encoding='utf-8') as file:\n        agent_yaml = yaml.safe_load(file)\n\n    # step 3.3: load the season yaml file\n    season_data = None    \n    with open(season_file_path, \"r\", encoding=\"utf-8\") as file:\n        season_data = yaml.safe_load(file)\n\n    # step 3.4: Generate a new season name, topic, and communication style with the prompt_2 template\n    season_template = manager.create_new_template_yaml(TemplateType.SEASON)\n\n    # step 3.5: we only want to send the details about the season and not all the episodes\n    season_template[\"season\"][\"season_number\"] = season_data[\"season\"][\"season_number\"]\n    season_template[\"season\"][\"season_name\"] = season_data[\"season\"][\"season_name\"]\n    season_template[\"season\"][\"season_description\"] = season_data[\"season\"][\"season_description\"]\n    season_template[\"season\"][\"season_highlights\"] = season_data[\"season\"][\"season_highlights\"]\n    season_template[\"season\"][\"season_summary\"] = season_data[\"season\"][\"season_summary\"]   \n\n    # step 3.6: Access the 'episodes' key under 'season'\n    episodes = season_data[\"season\"][\"episodes\"]\n    previous_episode = None\n    # step 3.7: Loop through each episode in the list\n    for episode in episodes:\n\n        # check to see if we already have created posts for this episode\n        # if we do, skip to the next episode\n        # check by file name\n        try:\n            # Get the project root directory (notice we're adding arai_ai_agents to match the actual path)\n            project_root = os.path.join(\n                os.path.dirname(os.path.dirname(os.path.dirname(__file__))),\n                \"arai_ai_agents\"\n            )\n\n            # Create file name using the exact same path as shown in the output\n            episode_file_path = os.path.join(\n                project_root,\n                \"configs\",\n                agent_yaml['name'],\n                f\"season_{season_template['season']['season_number']}\",\n                f\"s{season_template['season']['season_number']}_episode_{episode['episode_number']}.yaml\"\n            )\n\n            print(f\"Checking for existing episode at: {episode_file_path}\")  # Debug print\n\n            if os.path.exists(episode_file_path):\n                print(f\"Episode {episode['episode_number']} already exists at {episode_file_path}, skipping...\")\n                with open(episode_file_path, 'r', encoding='utf-8') as f:\n                    previous_episode = yaml.safe_load(f)\n                continue\n\n        except Exception as e:\n            print(f\"Error checking episode existence: {str(e)}\")\n            print(f\"Current working directory: {os.getcwd()}\")  # Debug print\n            pass\n\n        # step 3.8: create a new episode template\n        episode_template = manager.create_new_template_yaml(TemplateType.EPISODE)\n\n        # step 3.9: file in template\n        episode_template[\"episode\"][\"season_number\"] = season_data[\"season\"][\"season_number\"]\n        episode_template[\"episode\"][\"episode_number\"] = episode[\"episode_number\"]\n        episode_template[\"episode\"][\"episode_name\"] = episode[\"episode_name\"]\n        episode_template[\"episode\"][\"episode_description\"] = episode[\"episode_description\"]\n        episode_template[\"episode\"][\"episode_summary\"] = episode[\"episode_summary\"]\n        episode_template[\"episode\"][\"episode_highlights\"] = episode[\"episode_highlights\"]\n        episode_template[\"episode\"][\"episode_posted\"] = episode[\"episode_posted\"]     \n\n        # step 3.10: Generate a new season name, topic, and communication style with the prompt_2 template\n        # prompt 2 Season Creation:\n        # note that emojis will be output as unicode characters due to the yaml dump\n        prompt_3_vars = {\n            \"agent_name\": agent_yaml[\"name\"],\n            \"agent_yaml\": yaml.dump(agent_yaml),\n            \"season_yaml\": yaml.dump(season_template),\n            \"episode_yaml\": yaml.dump(episode_template),\n            \"previous_episode\": yaml.dump(previous_episode),\n            \"number_of_posts\": 12,\n            \"post_length\": 277\n        }\n\n        # step 3.11: Run the prompt \n        episode_data = manager.run_prompt(\n            # prompt_key=\"prompt_1 (Character Creation)\",\n            prompt_key=\"prompt_3 (Episode Posts Creation)\",\n            template_vars=prompt_3_vars, \n            ai_model=ai_model,\n        )\n\n        # step 3.12: Add the season data to the season template\n        episode_template = manager.add_data_to_template(\n            current_data=episode_template,\n            new_data=episode_data, \n        )\n\n        # step 3.13: create the file path\n        episode_file_path = manager.create_filepath(\n            agent_name=agent_yaml[\"name\"],\n            season_number=season_template[\"season\"][\"season_number\"],\n            episode_number=episode_template[\"episode\"][\"episode_number\"],\n            template_type=TemplateType.EPISODE\n        )\n\n        # step 3.14: Save the season data to a file\n        manager.save_yaml_file(\n            save_path=episode_file_path,\n            yaml_data=episode_template\n        )\n\n        # step 3.15: Move onto the next step of creating a new season\n        previous_episode = episode_template\n</code></pre>"},{"location":"api/utils/","title":"utils","text":""},{"location":"api/utils/#arai_ai_agents.utils","title":"<code>arai_ai_agents.utils</code>","text":""},{"location":"api/utils/content_generator/","title":"content_generator","text":""},{"location":"api/utils/content_generator/#arai_ai_agents.utils.content_generator","title":"<code>arai_ai_agents.utils.content_generator</code>","text":""},{"location":"api/utils/content_generator/#arai_ai_agents.utils.content_generator.ContentGenerator","title":"<code>ContentGenerator</code>","text":"Description <p>This class is responsible for generating content for the agents.</p> <p>Attributes:</p> Name Type Description <code>agents_config_dir</code> <code>str</code> <p>the directory to save the agent configurations</p> <code>templates_dir</code> <code>str</code> <p>the directory to save the agent templates</p> <code>agent_template_path</code> <code>str</code> <p>the path to the agent template</p> <code>chain_prompts_path</code> <code>str</code> <p>the path to the chain prompts</p> Source code in <code>arai_ai_agents/utils/content_generator.py</code> <pre><code>class ContentGenerator:\n    \"\"\"\n    Description:\n        This class is responsible for generating content for the agents.\n\n    Attributes:\n        agents_config_dir (str): the directory to save the agent configurations\n        templates_dir (str): the directory to save the agent templates\n        agent_template_path (str): the path to the agent template\n        chain_prompts_path (str): the path to the chain prompts\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"Initialize the ContentGenerator class.\n\n        Example:\n            &gt;&gt;&gt; content_generator = ContentGenerator()            \n        \"\"\"\n        # use relative path to get to project root (two levels up from utils)\n        project_root = os.path.dirname(os.path.dirname(__file__))\n\n        # Set the directories relative to project root\n        self.agents_config_dir = os.path.join(project_root, \"configs\")        \n        self.prompts_dir = os.path.join(project_root, \"prompts\")        \n\n        # Set the path for the chain prompt\n        self.chain_prompts_path = os.path.join(self.prompts_dir, \"prompt_chaining.yaml\")       \n\n        # Set the paths for the templates\n        self.templates_dir = os.path.join(project_root, \"templates\")\n        self.agent_template_path = os.path.join(self.templates_dir, \"agent_template.yaml\")\n        self.season_template_path = os.path.join(self.templates_dir, \"season_template.yaml\")\n        self.episode_template_path = os.path.join(self.templates_dir, \"episode_template.yaml\")\n\n\n    # -------------------------------------------------------------------\n    # Helper to create a new agent yaml file\n    # -------------------------------------------------------------------\n    def create_new_template_yaml(self, template_type: TemplateType) -&gt; dict:\n        \"\"\"Create a new agent configuration based on the template configuration file.\n\n        Args:\n            template_type (TemplateType): the type of template to create\n\n        Returns:\n            dict: the new agent configuration\n\n        Raises:\n            ValueError: If the template type is invalid\n\n        Example:\n            &gt;&gt;&gt; agent_config = create_new_agent_yaml()\n            &gt;&gt;&gt; print(agent_config)\n        \"\"\"\n        # 1. Ensure directory exist\n        os.makedirs(self.templates_dir, exist_ok=True)\n\n        # 2. Load the template configuration file\n        if template_type == TemplateType.AGENT:\n            template_path = self.agent_template_path\n        elif template_type == TemplateType.SEASON:\n            template_path = self.season_template_path\n        elif template_type == TemplateType.EPISODE:\n            template_path = self.episode_template_path\n        else:\n            raise ValueError(f\"Invalid template type: {template_type}\")\n\n        # 3. Load the template configuration file\n        with open(template_path, \"r\") as f:\n            template = yaml.safe_load(f)\n\n        # 3. Create a new configuration based on the template\n        new_config_file = template.copy()\n\n        # 4. Return new configuration to be processed by the agent_creator\n        return new_config_file\n\n    # -------------------------------------------------------------------\n    # Helper to safely parse YAML from the LLM's response\n    # -------------------------------------------------------------------\n    def process_and_save_agent_response(self, response) -&gt; dict:\n        \"\"\"Attempts to parse YAML from LLM text. \n\n        Args:\n            response (str): the response from the LLM\n            debug (bool, optional): whether to print debug information. Defaults to False.\n\n        Returns:\n            dict: the parsed YAML\n\n        Raises:\n            Exception: If there's an error parsing the YAML\n\n        Example:\n            &gt;&gt;&gt; response = \"```yaml\\nname: John Doe\\nage: 30\\n```\"\n            &gt;&gt;&gt; parsed = parse_yaml_from_response(response)\n            &gt;&gt;&gt; print(parsed)\n        \"\"\"\n\n        # 1. response = self.fix_yaml_from_response(response, debug)\n        raw_save_path = self.save_raw_response(response)\n        print(f\"raw_save_path is: {raw_save_path}\")\n\n        # 2. response = self.save_processed_response(response, debug)\n        save_path = self.create_yaml_from_response(response)       \n        print(f\"save_path is: {save_path}\")\n\n        # 3. load the yaml file into a dict\n        with open(save_path, \"r\", encoding=\"utf-8\") as f:\n            try:\n                # 3.1 load the yaml file into a dict\n                response = yaml.safe_load(f)                               \n            except Exception as e:\n                print(f\"process_and_save_agent_response. Error loading yaml file: {str(e)}\")                \n                return None        \n\n        # 4. Agent directory\n        file_dir = os.path.join(self.agents_config_dir, \"temporary\")\n        print(f\"file_dir is: {file_dir}\")\n\n        # 5. Make sure the agent directory exists\n        os.makedirs(file_dir, exist_ok=True)\n\n        # 6. Move files to agent directory                \n        saved_raw_path = self.move_file(raw_save_path, file_dir)\n        saved_processed_path = self.move_file(save_path, file_dir)\n\n        # 8. return the response\n        return response\n\n    # -------------------------------------------------------------------\n    # Helper to save the raw response to a file\n    # -------------------------------------------------------------------\n    def save_raw_response(self, response) -&gt; str:\n        \"\"\"Saves the raw response to a file.\n\n        Args:\n            response (str): the response from the LLM\n            debug (bool, optional): whether to print debug information. Defaults to False.\n\n        Returns:\n            str: the path to the saved yaml file\n\n        Raises:\n            Exception: If there's an error saving the response\n\n        Example:\n            &gt;&gt;&gt; response = \"```yaml\\nname: John Doe\\nage: 30\\n```\"\n            &gt;&gt;&gt; save_path = save_raw_response(response)\n            &gt;&gt;&gt; print(save_path)\n        \"\"\"\n        # 1. create a file to save the response\n        save_path = os.path.join(self.agents_config_dir, \"raw_response.yaml\")\n\n        # 2. Ensure directory exists\n        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n\n        # 3. Save the response to the file\n        with open(save_path, \"w\", encoding=\"utf-8\") as f:\n            try:\n                f.write(response)                \n                return save_path\n            except Exception as e:\n                print(f\"Error saving response to file: {str(e)}\")\n                return None\n\n    # -------------------------------------------------------------------\n    # Helper to create a yaml file from LLM text\n    # -------------------------------------------------------------------\n    def create_yaml_from_response(self, response) -&gt; str:\n        \"\"\"Attempts to create a yaml file from LLM text. \n\n        Args:\n            response (str): the response from the LLM\n\n        Returns:\n            str: the path to the saved yaml file\n\n        Raises:\n            Exception: If there's an error saving the response\n\n        Example:\n            &gt;&gt;&gt; response = \"```yaml\\nname: John Doe\\nage: 30\\n```\"\n            &gt;&gt;&gt; save_path = create_yaml_from_response(response)\n            &gt;&gt;&gt; print(save_path)\n        \"\"\"\n        # 1. strip out '''yaml and ''' \n        # remove new lines and leading and trailing whitespace\n        response = response.replace(\"```yaml\", \"\").replace(\"```\", \"\").strip()\n\n        # 2. create a file to save the response\n        save_path = os.path.join(self.agents_config_dir, \"processed_response.yaml\")\n\n        # 3. Ensure directory exists\n        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n\n        # 4. Save the response to the file\n        with open(save_path, \"w\", encoding=\"utf-8\") as f:\n            try:                \n                # yaml_string = yaml.dump(response, allow_unicode=True, default_flow_style=False)\n                # f.write(yaml_string)\n                f.write(response)\n            except Exception as e:\n                print(f\"Error saving response to file: {str(e)}\")      \n\n        return save_path\n\n    # -------------------------------------------------------------------\n    # Helper to rename file\n    # -------------------------------------------------------------------\n    def rename_file(self, old_path, new_name) -&gt; str:\n        \"\"\"Renames a file.\n\n        Args:\n            old_path (str): the old path to the file\n            new_name (str): the new name of the file\n\n        Returns:\n            str: the new path to the file\n\n        Raises:\n            Exception: If there's an error renaming the file\n\n        Example:\n            &gt;&gt;&gt; rename_file(\"tests/test.yaml\", \"test_new.yaml\")\n        \"\"\"\n        # 1. Ensure directory exists\n        os.makedirs(os.path.dirname(old_path), exist_ok=True)\n\n        # 2. Rename the file\n        try:\n            new_path = os.path.join(os.path.dirname(old_path), new_name)\n            os.rename(old_path, new_path)\n            return new_path \n        except Exception as e:\n            print(f\"Error renaming file: {str(e)}\")\n            return None\n\n    # -------------------------------------------------------------------\n    # Helper to move file\n    # -------------------------------------------------------------------\n    def move_file(self, old_path, new_dir) -&gt; str:\n        \"\"\"Moves a file.\n\n        Args:\n            old_path (str): the old path to the file\n            new_dir (str): the new directory to move the file to\n\n        Returns:\n            str: the new path to the file after moving\n\n        Raises:\n            Exception: If there's an error moving the file\n\n        Example:\n            &gt;&gt;&gt; move_file(\"tests/test.yaml\", \"agents/test\")\n        \"\"\"\n        # 1. Ensure directory exists\n        os.makedirs(new_dir, exist_ok=True)\n\n        # 2. Move the file\n        try:\n            new_path = os.path.join(new_dir, os.path.basename(old_path))\n            new_path = shutil.move(old_path, new_path)\n            return new_path\n        except Exception as e:\n            print(f\"Error moving file: {str(e)}\")\n            return None\n\n\n    # -------------------------------------------------------------------\n    # Helper to add new agent data to the current agent data\n    # -------------------------------------------------------------------\n    def add_data_to_template(self, current_data, new_data) -&gt; dict:\n        \"\"\"Adds new agent data to the current agent data.\n\n        Args:\n            new_data (dict): the new data\n            current_data (dict): the current data\n\n        Returns:\n            dict: the updated agent data\n\n        Raises:\n            Exception: If there's an error adding the data\n\n        Example:\n            &gt;&gt;&gt; new_data = {\"name\": \"John Doe\", \"age\": 30}\n            &gt;&gt;&gt; current_data = {\"name\": \"Jane Doe\", \"age\": 25}\n            &gt;&gt;&gt; updated_data = add_data_to_template(new_data, current_data)\n            &gt;&gt;&gt; print(updated_data)\n        \"\"\"\n        # 1. ensure we have a dictionary to work with\n        if isinstance(current_data, str):\n            existing_data = yaml.safe_load(current_data)\n        elif isinstance(current_data, dict):\n            existing_data = current_data.copy()\n        else:\n            existing_data = {}\n\n        # 2. Only update fields that already exist in existing_data\n        for key in new_data:\n            if key in existing_data:\n                existing_data[key] = new_data[key]\n\n        # 3. return the updated data\n        return existing_data\n\n    # -------------------------------------------------------------------\n    # Helper to create filepath\n    # -------------------------------------------------------------------\n    def create_filepath(self, agent_name: str, season_number: int, episode_number: int, template_type: TemplateType):\n        \"\"\"Creates a filepath for the agent data.\n\n        Args:\n            agent_name (str): the name of the agent\n            season_number (str): the number of the season\n            episode_number (str): the number of the episode\n            template_type (TemplateType): the type of template\n\n        Returns:\n            str: the filepath\n\n        Raises:\n            Exception: If there's an error creating the filepath\n\n        Example:\n            &gt;&gt;&gt; create_filepath(\"John Doe\", \"0\", TemplateType.AGENT)\n        \"\"\"\n        # 1. create the filepath based on the template type\n        if template_type == TemplateType.AGENT:            \n            return os.path.join(self.agents_config_dir, agent_name, agent_name + \".yaml\")\n        elif template_type == TemplateType.SEASON:\n            return os.path.join(self.agents_config_dir, agent_name, \"season_\" + str(season_number), \"season_\" + str(season_number) + \".yaml\")\n        elif template_type == TemplateType.EPISODE:\n            return os.path.join(self.agents_config_dir, agent_name, \"season_\" + str(season_number), \"s\" + str(season_number) + \"_episode_\" + str(episode_number) + \".yaml\")\n\n    # -------------------------------------------------------------------\n    # Helper to save the agent data to a yaml file\n    # -------------------------------------------------------------------\n    def save_yaml_file(self, save_path: str, yaml_data: dict):\n        \"\"\"Saves the agent data to a yaml file.\n\n        Args:\n            save_path (str): The path to save the YAML file\n            yaml_data (dict): The data to save to the YAML file\n\n        Returns:\n            str: the path to the saved yaml file\n\n        Raises:\n            Exception: If there's an error saving the yaml file\n\n        Example:\n            &gt;&gt;&gt; agent_data = {\"name\": \"John Doe\", \"age\": 30}\n            &gt;&gt;&gt; save_yaml_file(filepath=\"tests/test.yaml\", yaml_data=agent_data)\n        \"\"\"              \n        # 1. Ensure directory exists\n        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n\n        # 2. Save the response to the file\n        with open(save_path, \"w\", encoding=\"utf-8\") as f:\n            try:\n                # Convert dictionary to YAML string, then write directly to persver emoji images\n                # Others will get unicodes instead of the emojis\n                yaml_string = yaml.dump(yaml_data, allow_unicode=True, default_flow_style=False)\n                f.write(yaml_string)\n                print(f\"save_path is: {save_path}\")\n                return save_path\n            except Exception as e:\n                print(f\"Error saving response to file: {str(e)}\")\n                return None\n\n    # -------------------------------------------------------------------\n    # Generic prompt runner that works with any prompt template\n    # -------------------------------------------------------------------\n    def run_prompt(self, prompt_key, template_vars, ai_model, debug=False):\n        \"\"\"Generic prompt runner that works with any prompt template.\n\n        Args:\n            prompt_key (str): The key for the prompt template (e.g., \"prompt_1\", \"prompt_2\")\n            template_vars (dict): dict of variables to pass to the template\n            ai_model (ModelInterface): The AI model to use for generating responses\n            debug (bool, optional): whether to print debug information. Defaults to False.\n\n        Returns:\n            dict: the parsed YAML\n\n        Raises:\n            Exception: If there's an error running the prompt\n\n        Example:\n            &gt;&gt;&gt; prompt_key = \"prompt_1\"\n            &gt;&gt;&gt; template_vars = {\"name\": \"John Doe\", \"age\": 30}\n            &gt;&gt;&gt; ai_model = OpenAI(api_key=\"your_api_key\")\n            &gt;&gt;&gt; parsed = run_prompt(prompt_key, template_vars, ai_model, debug=True)\n            &gt;&gt;&gt; print(parsed)\n        \"\"\"\n        # 1. Load the chain prompts from the YAML file\n        with open(self.chain_prompts_path, \"r\", encoding=\"utf-8\") as f:\n            chain_prompts = yaml.safe_load(f)\n\n        # 2. Grab the raw prompt template text\n        prompt_template = chain_prompts[prompt_key]\n\n        # 3. Use Jinja2 to fill placeholders\n        template = Template(prompt_template)\n        prompt_text = template.render(**template_vars)\n\n        if debug:\n            print(\"--------------------------------\")\n            print(f\"prompt key is:\")\n            print (prompt_key)\n            print(\"--------------------------------\")\n            print(f\"prompt text is:\")\n            print (prompt_text)\n            print(\"--------------------------------\")\n\n        # 4. Call the LLM\n        response = ai_model.generate_response(prompt_text)\n\n        if debug:\n            print(\"--------------------------------\")\n            print(f\"response is:\")\n            print(response)\n            print(\"--------------------------------\")\n\n        # # 5. Parse the YAML from the LLM's response\n        yaml_response = self.process_and_save_agent_response(response)\n\n        if debug:\n            print(\"--------------------------------\")\n            print(f\"yaml_response is:\")\n            print(yaml_response)\n            print(\"--------------------------------\")\n            print(f\"yaml_response is a {type(yaml_response)}\")\n            print(\"--------------------------------\")\n\n        if yaml_response is None:\n             # Handle parse error or fallback\n             print(f\"Error: LLM returned invalid YAML for {prompt_key}.\")\n             return None\n\n        # 6. return yaml_response\n        return yaml_response\n</code></pre>"},{"location":"api/utils/content_generator/#arai_ai_agents.utils.content_generator.ContentGenerator.__init__","title":"<code>__init__()</code>","text":"<p>Initialize the ContentGenerator class.</p> Example <p>content_generator = ContentGenerator()</p> Source code in <code>arai_ai_agents/utils/content_generator.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize the ContentGenerator class.\n\n    Example:\n        &gt;&gt;&gt; content_generator = ContentGenerator()            \n    \"\"\"\n    # use relative path to get to project root (two levels up from utils)\n    project_root = os.path.dirname(os.path.dirname(__file__))\n\n    # Set the directories relative to project root\n    self.agents_config_dir = os.path.join(project_root, \"configs\")        \n    self.prompts_dir = os.path.join(project_root, \"prompts\")        \n\n    # Set the path for the chain prompt\n    self.chain_prompts_path = os.path.join(self.prompts_dir, \"prompt_chaining.yaml\")       \n\n    # Set the paths for the templates\n    self.templates_dir = os.path.join(project_root, \"templates\")\n    self.agent_template_path = os.path.join(self.templates_dir, \"agent_template.yaml\")\n    self.season_template_path = os.path.join(self.templates_dir, \"season_template.yaml\")\n    self.episode_template_path = os.path.join(self.templates_dir, \"episode_template.yaml\")\n</code></pre>"},{"location":"api/utils/content_generator/#arai_ai_agents.utils.content_generator.ContentGenerator.add_data_to_template","title":"<code>add_data_to_template(current_data, new_data)</code>","text":"<p>Adds new agent data to the current agent data.</p> <p>Parameters:</p> Name Type Description Default <code>new_data</code> <code>dict</code> <p>the new data</p> required <code>current_data</code> <code>dict</code> <p>the current data</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>the updated agent data</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If there's an error adding the data</p> Example <p>new_data = {\"name\": \"John Doe\", \"age\": 30} current_data = {\"name\": \"Jane Doe\", \"age\": 25} updated_data = add_data_to_template(new_data, current_data) print(updated_data)</p> Source code in <code>arai_ai_agents/utils/content_generator.py</code> <pre><code>def add_data_to_template(self, current_data, new_data) -&gt; dict:\n    \"\"\"Adds new agent data to the current agent data.\n\n    Args:\n        new_data (dict): the new data\n        current_data (dict): the current data\n\n    Returns:\n        dict: the updated agent data\n\n    Raises:\n        Exception: If there's an error adding the data\n\n    Example:\n        &gt;&gt;&gt; new_data = {\"name\": \"John Doe\", \"age\": 30}\n        &gt;&gt;&gt; current_data = {\"name\": \"Jane Doe\", \"age\": 25}\n        &gt;&gt;&gt; updated_data = add_data_to_template(new_data, current_data)\n        &gt;&gt;&gt; print(updated_data)\n    \"\"\"\n    # 1. ensure we have a dictionary to work with\n    if isinstance(current_data, str):\n        existing_data = yaml.safe_load(current_data)\n    elif isinstance(current_data, dict):\n        existing_data = current_data.copy()\n    else:\n        existing_data = {}\n\n    # 2. Only update fields that already exist in existing_data\n    for key in new_data:\n        if key in existing_data:\n            existing_data[key] = new_data[key]\n\n    # 3. return the updated data\n    return existing_data\n</code></pre>"},{"location":"api/utils/content_generator/#arai_ai_agents.utils.content_generator.ContentGenerator.create_filepath","title":"<code>create_filepath(agent_name, season_number, episode_number, template_type)</code>","text":"<p>Creates a filepath for the agent data.</p> <p>Parameters:</p> Name Type Description Default <code>agent_name</code> <code>str</code> <p>the name of the agent</p> required <code>season_number</code> <code>str</code> <p>the number of the season</p> required <code>episode_number</code> <code>str</code> <p>the number of the episode</p> required <code>template_type</code> <code>TemplateType</code> <p>the type of template</p> required <p>Returns:</p> Name Type Description <code>str</code> <p>the filepath</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If there's an error creating the filepath</p> Example <p>create_filepath(\"John Doe\", \"0\", TemplateType.AGENT)</p> Source code in <code>arai_ai_agents/utils/content_generator.py</code> <pre><code>def create_filepath(self, agent_name: str, season_number: int, episode_number: int, template_type: TemplateType):\n    \"\"\"Creates a filepath for the agent data.\n\n    Args:\n        agent_name (str): the name of the agent\n        season_number (str): the number of the season\n        episode_number (str): the number of the episode\n        template_type (TemplateType): the type of template\n\n    Returns:\n        str: the filepath\n\n    Raises:\n        Exception: If there's an error creating the filepath\n\n    Example:\n        &gt;&gt;&gt; create_filepath(\"John Doe\", \"0\", TemplateType.AGENT)\n    \"\"\"\n    # 1. create the filepath based on the template type\n    if template_type == TemplateType.AGENT:            \n        return os.path.join(self.agents_config_dir, agent_name, agent_name + \".yaml\")\n    elif template_type == TemplateType.SEASON:\n        return os.path.join(self.agents_config_dir, agent_name, \"season_\" + str(season_number), \"season_\" + str(season_number) + \".yaml\")\n    elif template_type == TemplateType.EPISODE:\n        return os.path.join(self.agents_config_dir, agent_name, \"season_\" + str(season_number), \"s\" + str(season_number) + \"_episode_\" + str(episode_number) + \".yaml\")\n</code></pre>"},{"location":"api/utils/content_generator/#arai_ai_agents.utils.content_generator.ContentGenerator.create_new_template_yaml","title":"<code>create_new_template_yaml(template_type)</code>","text":"<p>Create a new agent configuration based on the template configuration file.</p> <p>Parameters:</p> Name Type Description Default <code>template_type</code> <code>TemplateType</code> <p>the type of template to create</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>the new agent configuration</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the template type is invalid</p> Example <p>agent_config = create_new_agent_yaml() print(agent_config)</p> Source code in <code>arai_ai_agents/utils/content_generator.py</code> <pre><code>def create_new_template_yaml(self, template_type: TemplateType) -&gt; dict:\n    \"\"\"Create a new agent configuration based on the template configuration file.\n\n    Args:\n        template_type (TemplateType): the type of template to create\n\n    Returns:\n        dict: the new agent configuration\n\n    Raises:\n        ValueError: If the template type is invalid\n\n    Example:\n        &gt;&gt;&gt; agent_config = create_new_agent_yaml()\n        &gt;&gt;&gt; print(agent_config)\n    \"\"\"\n    # 1. Ensure directory exist\n    os.makedirs(self.templates_dir, exist_ok=True)\n\n    # 2. Load the template configuration file\n    if template_type == TemplateType.AGENT:\n        template_path = self.agent_template_path\n    elif template_type == TemplateType.SEASON:\n        template_path = self.season_template_path\n    elif template_type == TemplateType.EPISODE:\n        template_path = self.episode_template_path\n    else:\n        raise ValueError(f\"Invalid template type: {template_type}\")\n\n    # 3. Load the template configuration file\n    with open(template_path, \"r\") as f:\n        template = yaml.safe_load(f)\n\n    # 3. Create a new configuration based on the template\n    new_config_file = template.copy()\n\n    # 4. Return new configuration to be processed by the agent_creator\n    return new_config_file\n</code></pre>"},{"location":"api/utils/content_generator/#arai_ai_agents.utils.content_generator.ContentGenerator.create_yaml_from_response","title":"<code>create_yaml_from_response(response)</code>","text":"<p>Attempts to create a yaml file from LLM text. </p> <pre><code>    Args:\n        response (str): the response from the LLM\n\n    Returns:\n        str: the path to the saved yaml file\n\n    Raises:\n        Exception: If there's an error saving the response\n\n    Example:\n        &gt;&gt;&gt; response = \"```yaml\n</code></pre> <p>name: John Doe age: 30 ```\"             &gt;&gt;&gt; save_path = create_yaml_from_response(response)             &gt;&gt;&gt; print(save_path)</p> Source code in <code>arai_ai_agents/utils/content_generator.py</code> <pre><code>def create_yaml_from_response(self, response) -&gt; str:\n    \"\"\"Attempts to create a yaml file from LLM text. \n\n    Args:\n        response (str): the response from the LLM\n\n    Returns:\n        str: the path to the saved yaml file\n\n    Raises:\n        Exception: If there's an error saving the response\n\n    Example:\n        &gt;&gt;&gt; response = \"```yaml\\nname: John Doe\\nage: 30\\n```\"\n        &gt;&gt;&gt; save_path = create_yaml_from_response(response)\n        &gt;&gt;&gt; print(save_path)\n    \"\"\"\n    # 1. strip out '''yaml and ''' \n    # remove new lines and leading and trailing whitespace\n    response = response.replace(\"```yaml\", \"\").replace(\"```\", \"\").strip()\n\n    # 2. create a file to save the response\n    save_path = os.path.join(self.agents_config_dir, \"processed_response.yaml\")\n\n    # 3. Ensure directory exists\n    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n\n    # 4. Save the response to the file\n    with open(save_path, \"w\", encoding=\"utf-8\") as f:\n        try:                \n            # yaml_string = yaml.dump(response, allow_unicode=True, default_flow_style=False)\n            # f.write(yaml_string)\n            f.write(response)\n        except Exception as e:\n            print(f\"Error saving response to file: {str(e)}\")      \n\n    return save_path\n</code></pre>"},{"location":"api/utils/content_generator/#arai_ai_agents.utils.content_generator.ContentGenerator.move_file","title":"<code>move_file(old_path, new_dir)</code>","text":"<p>Moves a file.</p> <p>Parameters:</p> Name Type Description Default <code>old_path</code> <code>str</code> <p>the old path to the file</p> required <code>new_dir</code> <code>str</code> <p>the new directory to move the file to</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>the new path to the file after moving</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If there's an error moving the file</p> Example <p>move_file(\"tests/test.yaml\", \"agents/test\")</p> Source code in <code>arai_ai_agents/utils/content_generator.py</code> <pre><code>def move_file(self, old_path, new_dir) -&gt; str:\n    \"\"\"Moves a file.\n\n    Args:\n        old_path (str): the old path to the file\n        new_dir (str): the new directory to move the file to\n\n    Returns:\n        str: the new path to the file after moving\n\n    Raises:\n        Exception: If there's an error moving the file\n\n    Example:\n        &gt;&gt;&gt; move_file(\"tests/test.yaml\", \"agents/test\")\n    \"\"\"\n    # 1. Ensure directory exists\n    os.makedirs(new_dir, exist_ok=True)\n\n    # 2. Move the file\n    try:\n        new_path = os.path.join(new_dir, os.path.basename(old_path))\n        new_path = shutil.move(old_path, new_path)\n        return new_path\n    except Exception as e:\n        print(f\"Error moving file: {str(e)}\")\n        return None\n</code></pre>"},{"location":"api/utils/content_generator/#arai_ai_agents.utils.content_generator.ContentGenerator.process_and_save_agent_response","title":"<code>process_and_save_agent_response(response)</code>","text":"<p>Attempts to parse YAML from LLM text. </p> <pre><code>    Args:\n        response (str): the response from the LLM\n        debug (bool, optional): whether to print debug information. Defaults to False.\n\n    Returns:\n        dict: the parsed YAML\n\n    Raises:\n        Exception: If there's an error parsing the YAML\n\n    Example:\n        &gt;&gt;&gt; response = \"```yaml\n</code></pre> <p>name: John Doe age: 30 ```\"             &gt;&gt;&gt; parsed = parse_yaml_from_response(response)             &gt;&gt;&gt; print(parsed)</p> Source code in <code>arai_ai_agents/utils/content_generator.py</code> <pre><code>def process_and_save_agent_response(self, response) -&gt; dict:\n    \"\"\"Attempts to parse YAML from LLM text. \n\n    Args:\n        response (str): the response from the LLM\n        debug (bool, optional): whether to print debug information. Defaults to False.\n\n    Returns:\n        dict: the parsed YAML\n\n    Raises:\n        Exception: If there's an error parsing the YAML\n\n    Example:\n        &gt;&gt;&gt; response = \"```yaml\\nname: John Doe\\nage: 30\\n```\"\n        &gt;&gt;&gt; parsed = parse_yaml_from_response(response)\n        &gt;&gt;&gt; print(parsed)\n    \"\"\"\n\n    # 1. response = self.fix_yaml_from_response(response, debug)\n    raw_save_path = self.save_raw_response(response)\n    print(f\"raw_save_path is: {raw_save_path}\")\n\n    # 2. response = self.save_processed_response(response, debug)\n    save_path = self.create_yaml_from_response(response)       \n    print(f\"save_path is: {save_path}\")\n\n    # 3. load the yaml file into a dict\n    with open(save_path, \"r\", encoding=\"utf-8\") as f:\n        try:\n            # 3.1 load the yaml file into a dict\n            response = yaml.safe_load(f)                               \n        except Exception as e:\n            print(f\"process_and_save_agent_response. Error loading yaml file: {str(e)}\")                \n            return None        \n\n    # 4. Agent directory\n    file_dir = os.path.join(self.agents_config_dir, \"temporary\")\n    print(f\"file_dir is: {file_dir}\")\n\n    # 5. Make sure the agent directory exists\n    os.makedirs(file_dir, exist_ok=True)\n\n    # 6. Move files to agent directory                \n    saved_raw_path = self.move_file(raw_save_path, file_dir)\n    saved_processed_path = self.move_file(save_path, file_dir)\n\n    # 8. return the response\n    return response\n</code></pre>"},{"location":"api/utils/content_generator/#arai_ai_agents.utils.content_generator.ContentGenerator.rename_file","title":"<code>rename_file(old_path, new_name)</code>","text":"<p>Renames a file.</p> <p>Parameters:</p> Name Type Description Default <code>old_path</code> <code>str</code> <p>the old path to the file</p> required <code>new_name</code> <code>str</code> <p>the new name of the file</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>the new path to the file</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If there's an error renaming the file</p> Example <p>rename_file(\"tests/test.yaml\", \"test_new.yaml\")</p> Source code in <code>arai_ai_agents/utils/content_generator.py</code> <pre><code>def rename_file(self, old_path, new_name) -&gt; str:\n    \"\"\"Renames a file.\n\n    Args:\n        old_path (str): the old path to the file\n        new_name (str): the new name of the file\n\n    Returns:\n        str: the new path to the file\n\n    Raises:\n        Exception: If there's an error renaming the file\n\n    Example:\n        &gt;&gt;&gt; rename_file(\"tests/test.yaml\", \"test_new.yaml\")\n    \"\"\"\n    # 1. Ensure directory exists\n    os.makedirs(os.path.dirname(old_path), exist_ok=True)\n\n    # 2. Rename the file\n    try:\n        new_path = os.path.join(os.path.dirname(old_path), new_name)\n        os.rename(old_path, new_path)\n        return new_path \n    except Exception as e:\n        print(f\"Error renaming file: {str(e)}\")\n        return None\n</code></pre>"},{"location":"api/utils/content_generator/#arai_ai_agents.utils.content_generator.ContentGenerator.run_prompt","title":"<code>run_prompt(prompt_key, template_vars, ai_model, debug=False)</code>","text":"<p>Generic prompt runner that works with any prompt template.</p> <p>Parameters:</p> Name Type Description Default <code>prompt_key</code> <code>str</code> <p>The key for the prompt template (e.g., \"prompt_1\", \"prompt_2\")</p> required <code>template_vars</code> <code>dict</code> <p>dict of variables to pass to the template</p> required <code>ai_model</code> <code>ModelInterface</code> <p>The AI model to use for generating responses</p> required <code>debug</code> <code>bool</code> <p>whether to print debug information. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>the parsed YAML</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If there's an error running the prompt</p> Example <p>prompt_key = \"prompt_1\" template_vars = {\"name\": \"John Doe\", \"age\": 30} ai_model = OpenAI(api_key=\"your_api_key\") parsed = run_prompt(prompt_key, template_vars, ai_model, debug=True) print(parsed)</p> Source code in <code>arai_ai_agents/utils/content_generator.py</code> <pre><code>def run_prompt(self, prompt_key, template_vars, ai_model, debug=False):\n    \"\"\"Generic prompt runner that works with any prompt template.\n\n    Args:\n        prompt_key (str): The key for the prompt template (e.g., \"prompt_1\", \"prompt_2\")\n        template_vars (dict): dict of variables to pass to the template\n        ai_model (ModelInterface): The AI model to use for generating responses\n        debug (bool, optional): whether to print debug information. Defaults to False.\n\n    Returns:\n        dict: the parsed YAML\n\n    Raises:\n        Exception: If there's an error running the prompt\n\n    Example:\n        &gt;&gt;&gt; prompt_key = \"prompt_1\"\n        &gt;&gt;&gt; template_vars = {\"name\": \"John Doe\", \"age\": 30}\n        &gt;&gt;&gt; ai_model = OpenAI(api_key=\"your_api_key\")\n        &gt;&gt;&gt; parsed = run_prompt(prompt_key, template_vars, ai_model, debug=True)\n        &gt;&gt;&gt; print(parsed)\n    \"\"\"\n    # 1. Load the chain prompts from the YAML file\n    with open(self.chain_prompts_path, \"r\", encoding=\"utf-8\") as f:\n        chain_prompts = yaml.safe_load(f)\n\n    # 2. Grab the raw prompt template text\n    prompt_template = chain_prompts[prompt_key]\n\n    # 3. Use Jinja2 to fill placeholders\n    template = Template(prompt_template)\n    prompt_text = template.render(**template_vars)\n\n    if debug:\n        print(\"--------------------------------\")\n        print(f\"prompt key is:\")\n        print (prompt_key)\n        print(\"--------------------------------\")\n        print(f\"prompt text is:\")\n        print (prompt_text)\n        print(\"--------------------------------\")\n\n    # 4. Call the LLM\n    response = ai_model.generate_response(prompt_text)\n\n    if debug:\n        print(\"--------------------------------\")\n        print(f\"response is:\")\n        print(response)\n        print(\"--------------------------------\")\n\n    # # 5. Parse the YAML from the LLM's response\n    yaml_response = self.process_and_save_agent_response(response)\n\n    if debug:\n        print(\"--------------------------------\")\n        print(f\"yaml_response is:\")\n        print(yaml_response)\n        print(\"--------------------------------\")\n        print(f\"yaml_response is a {type(yaml_response)}\")\n        print(\"--------------------------------\")\n\n    if yaml_response is None:\n         # Handle parse error or fallback\n         print(f\"Error: LLM returned invalid YAML for {prompt_key}.\")\n         return None\n\n    # 6. return yaml_response\n    return yaml_response\n</code></pre>"},{"location":"api/utils/content_generator/#arai_ai_agents.utils.content_generator.ContentGenerator.save_raw_response","title":"<code>save_raw_response(response)</code>","text":"<p>Saves the raw response to a file.</p> <pre><code>    Args:\n        response (str): the response from the LLM\n        debug (bool, optional): whether to print debug information. Defaults to False.\n\n    Returns:\n        str: the path to the saved yaml file\n\n    Raises:\n        Exception: If there's an error saving the response\n\n    Example:\n        &gt;&gt;&gt; response = \"```yaml\n</code></pre> <p>name: John Doe age: 30 ```\"             &gt;&gt;&gt; save_path = save_raw_response(response)             &gt;&gt;&gt; print(save_path)</p> Source code in <code>arai_ai_agents/utils/content_generator.py</code> <pre><code>def save_raw_response(self, response) -&gt; str:\n    \"\"\"Saves the raw response to a file.\n\n    Args:\n        response (str): the response from the LLM\n        debug (bool, optional): whether to print debug information. Defaults to False.\n\n    Returns:\n        str: the path to the saved yaml file\n\n    Raises:\n        Exception: If there's an error saving the response\n\n    Example:\n        &gt;&gt;&gt; response = \"```yaml\\nname: John Doe\\nage: 30\\n```\"\n        &gt;&gt;&gt; save_path = save_raw_response(response)\n        &gt;&gt;&gt; print(save_path)\n    \"\"\"\n    # 1. create a file to save the response\n    save_path = os.path.join(self.agents_config_dir, \"raw_response.yaml\")\n\n    # 2. Ensure directory exists\n    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n\n    # 3. Save the response to the file\n    with open(save_path, \"w\", encoding=\"utf-8\") as f:\n        try:\n            f.write(response)                \n            return save_path\n        except Exception as e:\n            print(f\"Error saving response to file: {str(e)}\")\n            return None\n</code></pre>"},{"location":"api/utils/content_generator/#arai_ai_agents.utils.content_generator.ContentGenerator.save_yaml_file","title":"<code>save_yaml_file(save_path, yaml_data)</code>","text":"<p>Saves the agent data to a yaml file.</p> <p>Parameters:</p> Name Type Description Default <code>save_path</code> <code>str</code> <p>The path to save the YAML file</p> required <code>yaml_data</code> <code>dict</code> <p>The data to save to the YAML file</p> required <p>Returns:</p> Name Type Description <code>str</code> <p>the path to the saved yaml file</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If there's an error saving the yaml file</p> Example <p>agent_data = {\"name\": \"John Doe\", \"age\": 30} save_yaml_file(filepath=\"tests/test.yaml\", yaml_data=agent_data)</p> Source code in <code>arai_ai_agents/utils/content_generator.py</code> <pre><code>def save_yaml_file(self, save_path: str, yaml_data: dict):\n    \"\"\"Saves the agent data to a yaml file.\n\n    Args:\n        save_path (str): The path to save the YAML file\n        yaml_data (dict): The data to save to the YAML file\n\n    Returns:\n        str: the path to the saved yaml file\n\n    Raises:\n        Exception: If there's an error saving the yaml file\n\n    Example:\n        &gt;&gt;&gt; agent_data = {\"name\": \"John Doe\", \"age\": 30}\n        &gt;&gt;&gt; save_yaml_file(filepath=\"tests/test.yaml\", yaml_data=agent_data)\n    \"\"\"              \n    # 1. Ensure directory exists\n    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n\n    # 2. Save the response to the file\n    with open(save_path, \"w\", encoding=\"utf-8\") as f:\n        try:\n            # Convert dictionary to YAML string, then write directly to persver emoji images\n            # Others will get unicodes instead of the emojis\n            yaml_string = yaml.dump(yaml_data, allow_unicode=True, default_flow_style=False)\n            f.write(yaml_string)\n            print(f\"save_path is: {save_path}\")\n            return save_path\n        except Exception as e:\n            print(f\"Error saving response to file: {str(e)}\")\n            return None\n</code></pre>"},{"location":"api/utils/post_manager/","title":"post_manager","text":""},{"location":"api/utils/post_manager/#arai_ai_agents.utils.post_manager","title":"<code>arai_ai_agents.utils.post_manager</code>","text":""},{"location":"api/utils/post_manager/#arai_ai_agents.utils.post_manager.PostManager","title":"<code>PostManager</code>","text":"<p>Manages the post manager.</p> <p>Attributes:</p> Name Type Description <code>agent_name</code> <code>str</code> <p>The name of the agent</p> <code>root_folder</code> <code>str</code> <p>The root folder</p> <code>agent_folder</code> <code>str</code> <p>The agent folder</p> <code>season_folder</code> <code>str</code> <p>The season folder</p> <code>episode_folder</code> <code>str</code> <p>The episode folder</p> <code>tracker_file</code> <code>str</code> <p>The tracker file</p> <code>season_file</code> <code>str</code> <p>The season file</p> <code>episode_file</code> <code>str</code> <p>The episode file</p> Source code in <code>arai_ai_agents/utils/post_manager.py</code> <pre><code>class PostManager:\n    \"\"\"Manages the post manager.\n\n    Attributes:\n        agent_name (str): The name of the agent\n        root_folder (str): The root folder\n        agent_folder (str): The agent folder\n        season_folder (str): The season folder\n        episode_folder (str): The episode folder\n        tracker_file (str): The tracker file\n        season_file (str): The season file\n        episode_file (str): The episode file\n    \"\"\"\n    def __init__(self, agent_name: str):\n        \"\"\"Initialize the post manager\n\n        Args:\n            agent_name (str): The name of the agent\n\n        Raises:\n            Exception: If there's an error initializing the post manager\n\n        Example:\n            &gt;&gt;&gt; post_manager = PostManager(\"ZorpTheAlien\")\n        \"\"\"\n\n        # step 1: get the twitter connector\n        self.twitter_connector = twitter.TwitterConnector()\n\n        # step 2: get the agent name\n        self.agent_name = agent_name\n\n        # step 3: get the folders\n        self.root_folder = os.path.join(os.path.dirname(os.path.dirname(__file__)), \"configs\")\n        #self.agent_folder = os.path.join(self.root_folder, \"ZorpTheAlien\")\n        self.agent_folder = os.path.join(self.root_folder, self.agent_name)\n        self.season_folder = os.path.join(self.agent_folder, \"season_1\")\n        self.episode_folder = self.season_folder\n\n        print (f\"Agent folder: {self.agent_folder}\")\n\n        # step 4: get the tracker file\n        self.tracker_file = os.path.join(self.agent_folder, \"tracker.yaml\")\n\n        # Load the YAML file into a dictionary\n        with open(self.tracker_file, 'r', encoding='utf-8') as f:\n            self.tracker_data = yaml.safe_load(f)\n\n        # step 5: get the season file\n        self.season_file = os.path.join(self.season_folder, \"season_\" + str(self.tracker_data['current_season_number']) + \".yaml\")\n\n        # step 6: get the episode file\n        self.episode_file = os.path.join(self.episode_folder, \"s\" + str(self.tracker_data['current_season_number']) + \"_episode_\" + str(self.tracker_data['current_episode_number']) + \".yaml\")\n\n        # step 7: load season yaml file\n        with open(self.season_file, 'r', encoding='utf-8') as f:\n            self.season_data = yaml.safe_load(f)\n\n        # step 8: load episode yaml file\n        # load episode yaml file\n        with open(self.episode_file, 'r', encoding='utf-8') as f:\n            self.episode_data = yaml.safe_load(f)\n\n        # step 9: setup counters\n        # remember that code uses indexes starting at 0\n        # even if we have the number for the season, episode, and post starting at 1\n        self.season_number = self.tracker_data['current_season_number']\n        self.episode_number = self.tracker_data['current_episode_number']\n        self.post_number = self.tracker_data['current_post_number']\n\n        # step 10: print the counters\n        print (\"post_number: \", self.post_number)\n        print (\"episode_number: \", self.episode_number)\n        print (\"season_number: \", self.season_number)\n\n\n    def change_season(self, season_number: int):\n        \"\"\"Change the season\n\n        Args:\n            season_number (int): The season number\n\n        Example:\n            &gt;&gt;&gt; post_manager.change_season(1)\n        \"\"\"\n\n        print(f\"Changing season to ({season_number} + 1)\")\n        self.season_file = f\"configs/{self.agent_name}/season_{season_number}/season_{season_number}.yaml\"\n\n        with open(self.season_file, 'r', encoding='utf-8') as f:\n            self.season_data = yaml.safe_load(f)\n\n        # save the tracker data\n        self.tracker_data['current_season_number'] = season_number\n        with open(self.tracker_file, 'w', encoding='utf-8') as f:\n            yaml.dump(self.tracker_data, f)\n\n        print(f\"Changed season to {self.season_file}\")\n\n    def change_episode(self, episode_number: int):  \n        \"\"\"Change the episode\n\n        Args:\n            episode_number (int): The episode number\n\n        Example:\n            &gt;&gt;&gt; post_manager.change_episode(1)\n        \"\"\"\n\n        if episode_number &gt;= 28-1:\n            self.change_season(self.season_number + 1)            \n            self.episode_number = 1\n\n        self.episode_file = f\"configs/{self.agent_name}/season_1/s1_episode_{episode_number}.yaml\"\n\n        with open(self.episode_file, 'r', encoding='utf-8') as f:\n            self.episode_data = yaml.safe_load(f)\n\n        # save the tracker data\n        self.tracker_data['current_episode_number'] = episode_number\n        with open(self.tracker_file, 'w', encoding='utf-8') as f:\n            yaml.dump(self.tracker_data, f)\n\n        print(f\"Changed episode to {self.episode_file}\")\n\n    def next_post_number(self, post_number: int):\n        \"\"\"Change the post number\n\n        Args:\n            post_number (int): The post number\n\n        Returns:\n            str: The post content\n\n        Raises:\n            Exception: If there's an error changing the post number\n\n        Example:\n            &gt;&gt;&gt; post_content = post_manager.change_post_number(1)\n        \"\"\"\n\n        if post_number &gt;= 12-1:\n            self.change_episode(self.episode_number + 1)\n            self.post_number = 0\n\n        # Get the post content and clean it up, remember post numbers vs index starting at 0\n        post_content = self.episode_data['episode']['posts'][post_number]['post_content']\n        post_content = ' '.join(post_content.split()).strip()\n        # Convert Unicode escape sequences to emojis\n        post_content = post_content.encode().decode('unicode-escape')\n\n        # save the tracker data\n        self.tracker_data['current_post_number'] = self.post_number\n        with open(self.tracker_file, 'w', encoding='utf-8') as f:\n            yaml.dump(self.tracker_data, f)\n\n        # increase post number\n        self.post_number += 1        \n\n        return post_content\n\n    def post_to_twitter(self, live_post: bool = False):\n        \"\"\"Post to twitter\n\n        Args:\n            live_post (bool, optional): Whether to post to twitter. Defaults to False.\n\n        Raises:\n            Exception: If there's an error posting to twitter\n\n        Example:\n            &gt;&gt;&gt; post_manager.post_to_twitter()\n        \"\"\"\n        # print (\"Preparing to post to twitter\" + \"\\n\")\n\n        # get the next post number\n        tweet_content = self.next_post_number(self.post_number)\n\n        # if live_post is true, post to twitter\n        if live_post:\n            tweet_result = self.twitter_connector.post_tweet(tweet_content)\n\n            print (tweet_result)        \n\n            # reset the post number if the tweet failed so the post can be attempted again\n            if tweet_result.startswith(\"Error\"):\n                self.post_number -= 1        \n\n        # save to log file\n        try:\n            # First, read existing log if it exists\n            log_file = os.path.join(self.agent_folder, f\"{self.agent_name}_post_log.yaml\")\n            try:\n                with open(log_file, 'r', encoding='utf-8') as f:\n                    log_data = yaml.safe_load(f) or {'posts': []}\n            except FileNotFoundError:\n                log_data = {'posts': []}\n\n            # Add new post to the array\n            log_data['posts'].append({\n                'post_id': f\"s_{self.season_number}_e_{self.episode_number}_p_{self.post_number}\",\n                'content': tweet_content,\n                'timestamp': str(datetime.datetime.now())\n            })\n\n            # Write back the updated log\n            with open(log_file, 'w', encoding='utf-8') as f:\n                yaml.dump(log_data, f, allow_unicode=True)\n\n        except Exception as e:\n            print(f\"Error writing to log file: {e}\")        \n</code></pre>"},{"location":"api/utils/post_manager/#arai_ai_agents.utils.post_manager.PostManager.__init__","title":"<code>__init__(agent_name)</code>","text":"<p>Initialize the post manager</p> <p>Parameters:</p> Name Type Description Default <code>agent_name</code> <code>str</code> <p>The name of the agent</p> required <p>Raises:</p> Type Description <code>Exception</code> <p>If there's an error initializing the post manager</p> Example <p>post_manager = PostManager(\"ZorpTheAlien\")</p> Source code in <code>arai_ai_agents/utils/post_manager.py</code> <pre><code>def __init__(self, agent_name: str):\n    \"\"\"Initialize the post manager\n\n    Args:\n        agent_name (str): The name of the agent\n\n    Raises:\n        Exception: If there's an error initializing the post manager\n\n    Example:\n        &gt;&gt;&gt; post_manager = PostManager(\"ZorpTheAlien\")\n    \"\"\"\n\n    # step 1: get the twitter connector\n    self.twitter_connector = twitter.TwitterConnector()\n\n    # step 2: get the agent name\n    self.agent_name = agent_name\n\n    # step 3: get the folders\n    self.root_folder = os.path.join(os.path.dirname(os.path.dirname(__file__)), \"configs\")\n    #self.agent_folder = os.path.join(self.root_folder, \"ZorpTheAlien\")\n    self.agent_folder = os.path.join(self.root_folder, self.agent_name)\n    self.season_folder = os.path.join(self.agent_folder, \"season_1\")\n    self.episode_folder = self.season_folder\n\n    print (f\"Agent folder: {self.agent_folder}\")\n\n    # step 4: get the tracker file\n    self.tracker_file = os.path.join(self.agent_folder, \"tracker.yaml\")\n\n    # Load the YAML file into a dictionary\n    with open(self.tracker_file, 'r', encoding='utf-8') as f:\n        self.tracker_data = yaml.safe_load(f)\n\n    # step 5: get the season file\n    self.season_file = os.path.join(self.season_folder, \"season_\" + str(self.tracker_data['current_season_number']) + \".yaml\")\n\n    # step 6: get the episode file\n    self.episode_file = os.path.join(self.episode_folder, \"s\" + str(self.tracker_data['current_season_number']) + \"_episode_\" + str(self.tracker_data['current_episode_number']) + \".yaml\")\n\n    # step 7: load season yaml file\n    with open(self.season_file, 'r', encoding='utf-8') as f:\n        self.season_data = yaml.safe_load(f)\n\n    # step 8: load episode yaml file\n    # load episode yaml file\n    with open(self.episode_file, 'r', encoding='utf-8') as f:\n        self.episode_data = yaml.safe_load(f)\n\n    # step 9: setup counters\n    # remember that code uses indexes starting at 0\n    # even if we have the number for the season, episode, and post starting at 1\n    self.season_number = self.tracker_data['current_season_number']\n    self.episode_number = self.tracker_data['current_episode_number']\n    self.post_number = self.tracker_data['current_post_number']\n\n    # step 10: print the counters\n    print (\"post_number: \", self.post_number)\n    print (\"episode_number: \", self.episode_number)\n    print (\"season_number: \", self.season_number)\n</code></pre>"},{"location":"api/utils/post_manager/#arai_ai_agents.utils.post_manager.PostManager.change_episode","title":"<code>change_episode(episode_number)</code>","text":"<p>Change the episode</p> <p>Parameters:</p> Name Type Description Default <code>episode_number</code> <code>int</code> <p>The episode number</p> required Example <p>post_manager.change_episode(1)</p> Source code in <code>arai_ai_agents/utils/post_manager.py</code> <pre><code>def change_episode(self, episode_number: int):  \n    \"\"\"Change the episode\n\n    Args:\n        episode_number (int): The episode number\n\n    Example:\n        &gt;&gt;&gt; post_manager.change_episode(1)\n    \"\"\"\n\n    if episode_number &gt;= 28-1:\n        self.change_season(self.season_number + 1)            \n        self.episode_number = 1\n\n    self.episode_file = f\"configs/{self.agent_name}/season_1/s1_episode_{episode_number}.yaml\"\n\n    with open(self.episode_file, 'r', encoding='utf-8') as f:\n        self.episode_data = yaml.safe_load(f)\n\n    # save the tracker data\n    self.tracker_data['current_episode_number'] = episode_number\n    with open(self.tracker_file, 'w', encoding='utf-8') as f:\n        yaml.dump(self.tracker_data, f)\n\n    print(f\"Changed episode to {self.episode_file}\")\n</code></pre>"},{"location":"api/utils/post_manager/#arai_ai_agents.utils.post_manager.PostManager.change_season","title":"<code>change_season(season_number)</code>","text":"<p>Change the season</p> <p>Parameters:</p> Name Type Description Default <code>season_number</code> <code>int</code> <p>The season number</p> required Example <p>post_manager.change_season(1)</p> Source code in <code>arai_ai_agents/utils/post_manager.py</code> <pre><code>def change_season(self, season_number: int):\n    \"\"\"Change the season\n\n    Args:\n        season_number (int): The season number\n\n    Example:\n        &gt;&gt;&gt; post_manager.change_season(1)\n    \"\"\"\n\n    print(f\"Changing season to ({season_number} + 1)\")\n    self.season_file = f\"configs/{self.agent_name}/season_{season_number}/season_{season_number}.yaml\"\n\n    with open(self.season_file, 'r', encoding='utf-8') as f:\n        self.season_data = yaml.safe_load(f)\n\n    # save the tracker data\n    self.tracker_data['current_season_number'] = season_number\n    with open(self.tracker_file, 'w', encoding='utf-8') as f:\n        yaml.dump(self.tracker_data, f)\n\n    print(f\"Changed season to {self.season_file}\")\n</code></pre>"},{"location":"api/utils/post_manager/#arai_ai_agents.utils.post_manager.PostManager.next_post_number","title":"<code>next_post_number(post_number)</code>","text":"<p>Change the post number</p> <p>Parameters:</p> Name Type Description Default <code>post_number</code> <code>int</code> <p>The post number</p> required <p>Returns:</p> Name Type Description <code>str</code> <p>The post content</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If there's an error changing the post number</p> Example <p>post_content = post_manager.change_post_number(1)</p> Source code in <code>arai_ai_agents/utils/post_manager.py</code> <pre><code>def next_post_number(self, post_number: int):\n    \"\"\"Change the post number\n\n    Args:\n        post_number (int): The post number\n\n    Returns:\n        str: The post content\n\n    Raises:\n        Exception: If there's an error changing the post number\n\n    Example:\n        &gt;&gt;&gt; post_content = post_manager.change_post_number(1)\n    \"\"\"\n\n    if post_number &gt;= 12-1:\n        self.change_episode(self.episode_number + 1)\n        self.post_number = 0\n\n    # Get the post content and clean it up, remember post numbers vs index starting at 0\n    post_content = self.episode_data['episode']['posts'][post_number]['post_content']\n    post_content = ' '.join(post_content.split()).strip()\n    # Convert Unicode escape sequences to emojis\n    post_content = post_content.encode().decode('unicode-escape')\n\n    # save the tracker data\n    self.tracker_data['current_post_number'] = self.post_number\n    with open(self.tracker_file, 'w', encoding='utf-8') as f:\n        yaml.dump(self.tracker_data, f)\n\n    # increase post number\n    self.post_number += 1        \n\n    return post_content\n</code></pre>"},{"location":"api/utils/post_manager/#arai_ai_agents.utils.post_manager.PostManager.post_to_twitter","title":"<code>post_to_twitter(live_post=False)</code>","text":"<p>Post to twitter</p> <p>Parameters:</p> Name Type Description Default <code>live_post</code> <code>bool</code> <p>Whether to post to twitter. Defaults to False.</p> <code>False</code> <p>Raises:</p> Type Description <code>Exception</code> <p>If there's an error posting to twitter</p> Example <p>post_manager.post_to_twitter()</p> Source code in <code>arai_ai_agents/utils/post_manager.py</code> <pre><code>def post_to_twitter(self, live_post: bool = False):\n    \"\"\"Post to twitter\n\n    Args:\n        live_post (bool, optional): Whether to post to twitter. Defaults to False.\n\n    Raises:\n        Exception: If there's an error posting to twitter\n\n    Example:\n        &gt;&gt;&gt; post_manager.post_to_twitter()\n    \"\"\"\n    # print (\"Preparing to post to twitter\" + \"\\n\")\n\n    # get the next post number\n    tweet_content = self.next_post_number(self.post_number)\n\n    # if live_post is true, post to twitter\n    if live_post:\n        tweet_result = self.twitter_connector.post_tweet(tweet_content)\n\n        print (tweet_result)        \n\n        # reset the post number if the tweet failed so the post can be attempted again\n        if tweet_result.startswith(\"Error\"):\n            self.post_number -= 1        \n\n    # save to log file\n    try:\n        # First, read existing log if it exists\n        log_file = os.path.join(self.agent_folder, f\"{self.agent_name}_post_log.yaml\")\n        try:\n            with open(log_file, 'r', encoding='utf-8') as f:\n                log_data = yaml.safe_load(f) or {'posts': []}\n        except FileNotFoundError:\n            log_data = {'posts': []}\n\n        # Add new post to the array\n        log_data['posts'].append({\n            'post_id': f\"s_{self.season_number}_e_{self.episode_number}_p_{self.post_number}\",\n            'content': tweet_content,\n            'timestamp': str(datetime.datetime.now())\n        })\n\n        # Write back the updated log\n        with open(log_file, 'w', encoding='utf-8') as f:\n            yaml.dump(log_data, f, allow_unicode=True)\n\n    except Exception as e:\n        print(f\"Error writing to log file: {e}\")        \n</code></pre>"},{"location":"api/utils/template_types/","title":"template_types","text":""},{"location":"api/utils/template_types/#arai_ai_agents.utils.template_types","title":"<code>arai_ai_agents.utils.template_types</code>","text":""},{"location":"api/utils/template_types/#arai_ai_agents.utils.template_types.TemplateType","title":"<code>TemplateType</code>","text":"<p>This class is used to define the type of template.</p> <p>Attributes:</p> Name Type Description <code>AGENT</code> <code>str</code> <p>the type of template for an agent</p> <code>SEASON</code> <code>str</code> <p>the type of template for a season</p> <code>EPISODE</code> <code>str</code> <p>the type of template for an episode</p> <code>PROFILE_PICTURE</code> <code>str</code> <p>the type of template for a profile picture</p> Source code in <code>arai_ai_agents/utils/template_types.py</code> <pre><code>class TemplateType:\n    \"\"\"This class is used to define the type of template.\n\n    Attributes:\n        AGENT (str): the type of template for an agent\n        SEASON (str): the type of template for a season\n        EPISODE (str): the type of template for an episode\n        PROFILE_PICTURE (str): the type of template for a profile picture\n    \"\"\"\n    AGENT = \"agent\"\n    SEASON = \"season\"\n    EPISODE = \"episode\"\n    PROFILE_PICTURE = \"profile_picture\"\n</code></pre>"},{"location":"yaml/prompts/prompt_chaining/","title":"Prompt Chaining","text":"<pre><code>prompt_1 (Character Sheet Creation): |\n  You are an expert in creative writing, character design, world-building, and marketing. You are tasked with developing a complete character profile for a new agent who will be featured in stories for a Twitter bot. These stories will be broken down into seasons, episodes, and individual tweets (scenes/posts).\n\n  Agent Development Task\n  Generate a comprehensive YAML file that defines this new agent, including their name, personality, communication style, topic of expertise, backstory, universe, relevant hashtags, and emojis.\n  Use the following concept if its not empty, otherwise create one.\n  - concept: {{ concept }}\n\n  Specific Instructions:\n\n    1.  Invent a Creative Name:\n        - If no specific name is provided, create a unique and fitting name for the agent based on the concept of the agent. The name should be suitable for a Twitter handle.\n        - Agent Name: {{ agent_name }}\n\n    2.  Define the Topic:\n        - If no specific topic is provided, default to \"Crypto\" or invent a creative topic that lends itself to episodic storytelling.\n        - Topic: {{ topic }}\n\n    3.  Develop Personality and Style:\n        - Create a detailed personality for the agent.\n        - Define a clear communication style for the agent.\n        - Important: Use the YAML block scalar style with the `|` character to preserve newlines and formatting.\n        - Example:\n          ```yaml\n          personality: |\n            This is a multi-line\n            description of the agent's\n            personality.\n          ```\n        - Personality: {{ personality }}\n        - Communication Style: {{ style }}\n\n    4.  Craft a Backstory:\n        - Invent a compelling and original backstory for the agent that explains their motivations, skills, and current situation. This backstory should be suitable for unfolding gradually over multiple story arcs.\n        - Important: Use the YAML block scalar style with the `|` character to preserve newlines and formatting.\n        - Example:\n          ```yaml\n          backstory: |\n            This is a longer, multi-line\n            backstory for the agent.\n            It can span several lines.\n          ```\n        - Backstory: {{ backstory }}\n\n    5.  Describe the Universe:\n        - Detail the world(s) or setting where the agent operates. Consider the current state of technology, the social and political landscape, major organizations or factions, and any unique elements relevant to the chosen topic.\n        - Use a narrative or bullet-point format within the YAML.\n        - Important: Use the YAML block scalar style with the `|` character to preserve newlines and formatting.\n        - Example:\n          ```yaml\n          universe: |\n            This is a description\n            of the agent's universe.\n            It can have multiple lines\n            and paragraphs.\n          ```\n        - Universe: {{ universe }}\n\n    6.  Generate Marketing Elements:\n        - Create a list of relevant hashtags that will be used for social media promotion. Include hashtags related to the agent's name, topic, and genre.\n        - Important: Output the hashtags as a YAML array using block style. This means each hashtag should be on a new line, preceded by a hyphen and a space and placed inside double quotes.\n        - Example:\n          ```yaml\n          hashtags:\n            - \"#example1\"\n            - \"#example2\"\n            - \"#longerExampleHashtag\"\n          ```\n        - Create a list of relevant emojis that can be used in tweets to add visual interest and convey meaning.\n        - Important: Output the emojis as a YAML array using block style. Each emoji should be on a new line, preceded by a hyphen and a space and placed inside double quotes.\n        - Example:\n          ```yaml\n          emojis:\n            - \"\ud83d\udc33\"\n            - \"\ud83c\udf0a\"\n            - \"\ud83d\ude80\"\n          ```\n        - Hashtags: {{ hashtags }}\n        - Emojis: {{ emojis }}\n\n  Output Requirements:\n  - Only output valid YAML. Do not include any text outside of the YAML structure.\n  - Output a single, complete YAML file that includes all the fields mentioned above (name, personality, communication_style, topic, backstory, universe, hashtags, emojis).\n  - Adhere to the specified answer lengths for each field (short for personality and communication style, long for backstory and universe).\n\n  Output exactly as the yaml file:\n  ```yaml\n  {{ agent_yaml }}\n  ```\n\nprompt_2 (Season Creation): |\n  You are an expert in creative writing, season/episode design, and structured storytelling.\n  Your goal is to create a new season for {{ agent_name }}, who was previously defined in the following agent YAML:\n\n  {{ agent_yaml }}\n\n  Previous season:\n  - This provides the previous season details, including description and highlights for the season. Ignore if none is provided as this means we are creating season 1, so there is no previous season.\n  {{ previous_season }}\n\n  ---\n  ## Task: Season Creation\n\n  Generate a single valid YAML file that defines a new season for this agent. The season should include:\n\n  - Season Name: If none is provided, create a unique and fitting name based on {{ agent_name }} and the agent\u2019s YAML.\n      - Use something that can also work as a Twitter hashtag.\n      - Season Name: {{ season_name }}\n\n  - Season Number: If none is provided, default to 1.\n      - Season Number: {{ season_number }}\n\n  - Season Description: If none is provided, create a unique description based on {{ agent_name }}\u2019s YAML.\n      - Important: Use the YAML block scalar style with the `|` character to preserve newlines and formatting.\n      - Season Description: {{ season_description }}\n\n  - Season Highlights: If none is provided, invent highlights relevant to the agent\u2019s concept or storyline.\n      - Important: Use the YAML block scalar style with the `|` character to preserve newlines and formatting.\n      - Season Highlights: {{ season_highlights }}\n\n  - Season Summary: If none is provided, create a concise summary that captures the essence of this season.\n      - Important: Use the YAML block scalar style with the `|` character to preserve newlines and formatting.\n      - Season Summary: {{ season_summary }}\n\n  - Episodes: We want **28 episodes** total (one for each day over 4 weeks).\n      - If none are provided, create 28 unique episodes.\n      - Each episode entry should have:\n          - `episode_id`\n          - `episode_name`\n          - `episode_number`\n          - `episode_description`  (Use `|` for multi-line)\n          - `episode_highlights` (Use `|` for multi-line)\n          - `episode_posted` (default `False`)\n          - `episode_summary` (Use `|` for multi-line)\n\n  Output Requirements:\n  - Only output valid YAML. Do not include any text outside of the YAML structure.\n  - Output a single, complete YAML file that includes all the fields mentioned above (season_name, season_number, season_description, season_highlights, season_summary, episodes).\n\n  Output exactly as the yaml file:\n  ```yaml\n  {{ season_yaml }}\n\nprompt_3 (Episode Posts Creation): |\n  You are an expert in creative writing, social media engagement, and character voice development. Your task is to generate {{ number_of_posts }} short \"updates\" (tweets/posts) for the agent {{ agent_name }} in the style of Twitter posts. These posts are for season {{ season_number }}, episode {{ episode_number }}.\n\n  Agent and Context:\n\n  *   Agent YAML: {{ agent_yaml }} (This provides the agent's personality, backstory, etc.)\n  *   Season YAML: {{ season_yaml }} (This provides the season details, including description and highlights for the season)\n  *   Episode YAML: {{ episode_yaml }} (This provides the episode details, including description and highlights for the episode)\n  *   Previous Episode YAML: {{ previous_episode }} (This provides the previous episode details, including description and highlights for the episode. Ignore if none is provided as this means we are creating episode 1, so there is no previous episode)\n  **Post Requirements:**\n\n  Each post should:\n\n  *   Reflect Agent's Personality: Be consistent with {{ agent_name }}'s personality, tone, and communication style as defined in the agent YAML.\n  *   Relate to Setting/Technology:  Reference elements of the setting, technology, or other relevant details from the agent's universe as defined in the agent YAML.\n  *   Incorporate Episode Context: Align with the season overview and specific episode overview provided in the season YAML.\n  *   Highlight Observations: Center around misunderstandings, comedic observations, or serious reflections relevant to the episode's theme.\n  *   Consider Emojis: Optionally use emojis from this set: {{ agent_yaml }} or use any emojis that are relevant to the episode's theme.\n  *   Consider Hashtags: Optionally use hashtags from this set: {{ agent_yaml }} or use any hashtags that are relevant to the episode's theme.\n  *   Length Constraint: Each post must be under {{ post_length }} characters.\n  *   Sign-Off: End each post with the sign-off: \"#{{ agent_name }}\"\n\n  **YAML Output Format:**\n\n  Output a valid YAML file containing a list of posts under the `posts` key. Each post should be a separate entry in the list and include the following:\n\n  *   `post_id`: A unique identifier for the post (e.g., s1\\_e1\\_post1, s1\\_e1\\_post2).\n  *   `post_number`: The ordinal number of the post within the episode (e.g., 1, 2, 3).\n  *   `post_content`: The actual text of the post, formatted using the YAML block scalar style (`|`) to preserve newlines, do not use '-' for the first line.\n  *   `post_highlights`: A brief, one-sentence description of the key takeaway or event in the post (using `|` for multi-line), do not use '-' for the first line.\n  *   `post_posted`:  Set to `False` by default.\n\n  **Output Requirements:**\n  - **Only output valid YAML.** Do not include any text outside of the YAML structure.\n  - **Output the yaml file:**  \n  ```yaml\n  {{ episode_yaml }}\n  ``` \n</code></pre>"},{"location":"yaml/templates/agent_template/","title":"Agent Template","text":"<pre><code>name:\npersonality:\ncommunication_style:\nbackstory:\nuniverse:\ntopic_expertise:\nhashtags: \nemojis: \nmodel_type: \"gemini\"\nmodel_name: \"gemini-exp-1206\"\nmemory_store: \"gemini_chroma\"\nconnectors:\n  twitter: true\n  telegram: false\n  discord: false\n</code></pre>"},{"location":"yaml/templates/episode_template/","title":"Episode Template","text":"<pre><code>episode:\n  season_number:  \n  episode_name:\n  episode_number:\n  episode_overview: |\n  episode_summary: |\n  episode_highlights: |\n  episode_posted: False\n  current_post_number: 0\n  posts:\n    - post_id:\n      post_number:\n      post_content: |\n      post_highlights: |\n      post_posted: False\n</code></pre>"},{"location":"yaml/templates/season_template/","title":"Season Template","text":"<pre><code>season:\n  season_name:\n  season_number:\n  season_description: |\n  season_highlights: |\n  season_summary: |\n  season_posted: False\n  current_episode_number: 0\n  episodes:\n    episode_name:\n    episode_number:\n    episode_description: |\n    episode_highlights: |\n    episode_summary: |\n    episode_posted: False\n</code></pre>"},{"location":"yaml/templates/tracker_template/","title":"Tracker Template","text":"<pre><code>current_season_number: 1\ncurrent_episode_number: 1\ncurrent_post_number: 0\npost_every_x_minutes: 60\n</code></pre>"}]}